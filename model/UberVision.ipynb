{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimpy import skim\n",
    "from scipy import stats\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, SelectKBest, RFE\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO \n",
    "\n",
    "import ray\n",
    "import optuna\n",
    "from multiprocessing import Pool, get_context\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, ReLU, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UberVision:\n",
    "\n",
    "    # global path variable\n",
    "    data_connection = '/uber_eats/restaurants.csv'\n",
    "    author = 'Ben Stager, 2024'\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        return\n",
    "\n",
    "    # load csv into memory\n",
    "    def load_data(self, cols=''):  \n",
    "        self.cols = cols \n",
    "        if cols == '':\n",
    "            return pd.read_csv(UberVision.data_connection)\n",
    "\n",
    "    def load_image(self, url_arr):\n",
    "        self.url_arr = url_arr\n",
    "        \n",
    "        if url_arr is list():\n",
    "            self.url_arr = url_arr\n",
    "            images = []\n",
    "            print(url_arr)\n",
    "\n",
    "            for url in url_arr:\n",
    "                response = requests.get(url)\n",
    "                img = Image.open(BytesIO(response.content))\n",
    "                images.append(img)\n",
    "\n",
    "            return images\n",
    "        \n",
    "        self.url_arr = url_arr\n",
    "        images = []\n",
    "        print(url_arr)\n",
    "    \n",
    "    def extract_feature(self, img):\n",
    "        self.img_arr = img\n",
    "\n",
    "        resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "\n",
    "        features_img = resnet.predict(img_array)[0]\n",
    "\n",
    "        return features_img\n",
    "    \n",
    "    \n",
    "    def prepare_extract_features(self, img_arr):\n",
    "\n",
    "        self.img_arr = img_arr\n",
    "        features = []\n",
    "\n",
    "        resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "        for img in img_arr:\n",
    "            img_array = img_to_array(img)\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "            img_array = preprocess_input(img_array)\n",
    "\n",
    "            features_img = resnet.predict(img_array)\n",
    "\n",
    "            features.extend(features_img[0])\n",
    "\n",
    "        return features\n",
    "    \n",
    "    def UberBaseline():\n",
    "        pass\n",
    "        \n",
    "    def UberXGBoost():\n",
    "        pass\n",
    "\n",
    "    def UberRandomForest():\n",
    "        pass\n",
    "\n",
    "    def UberBaseline():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(url_arr):\n",
    "    images = []\n",
    "    print(url_arr)\n",
    "\n",
    "    for url in url_arr:\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        images.append(img)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_extract_features(img_arr):\n",
    "    features = []\n",
    "\n",
    "    resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "    for img in img_arr:\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "\n",
    "        features_img = resnet.predict(img_array)\n",
    "\n",
    "        features.extend(features_img[0])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_PCA(X_train, X_test, use_all=True, num_to_use=3, num_components=3):\n",
    "    \n",
    "    if use_all:\n",
    "        pca = PCA()\n",
    "        X_train_PCA = pca.fit_transform(X_train)[:,:num_to_use]\n",
    "        X_test_PCA = pca.transform(X_test)[:,:num_to_use]\n",
    "\n",
    "        return X_train_PCA, X_test_PCA\n",
    "    \n",
    "    pca = PCA(num_components)\n",
    "    return pca.fit_transform(X_train), pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_price(x):\n",
    "    prices_dict = {'$':0, '$$':1}\n",
    "    return prices_dict[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cols = []\n",
    "img_cols.append('loc_no_address')\n",
    "img_cols.append('loc_name')\n",
    "[img_cols.append(f'img{i}') for i in range(1,6)]\n",
    "img_cols.append('review_rating')\n",
    "img_cols.append('price_bucket')\n",
    "\n",
    "df = pd.read_csv('Ubereat_US_Merchant.csv')\n",
    "df['loc_name'] = df['loc_name'].apply(lambda x: x.lstrip())\n",
    "df['loc_name'] = df['loc_name'].apply(lambda x: x.rstrip())\n",
    "df = df.dropna(subset=img_cols[2:7])\n",
    "df['loc_no_address'] = df['loc_name'].apply(lambda x: x.split('(')[0])\n",
    "\n",
    "df = df[img_cols]\n",
    "\n",
    "df = df.groupby('loc_no_address').agg({'img1':'first','img2':'first','img3':'first','img4':'first','img5':'first','review_rating':'mean', 'price_bucket':'first'}).reset_index()\n",
    "\n",
    "df_labeled = df[df['review_rating'].isna() == False]\n",
    "df_unlabeled = df[df['review_rating'].isna() == True]\n",
    "\n",
    "features = pd.read_csv('design_matrix.csv')\n",
    "\n",
    "df_training = pd.concat([df_labeled.reset_index().drop('index',axis=1), features.reset_index().drop(['index', 'Unnamed: 0'],axis=1)],axis=1)\n",
    "\n",
    "one = df_training.drop('review_rating',axis=1)\n",
    "two = df_training['review_rating']\n",
    "\n",
    "df_training = pd.concat([one,two],axis=1)\n",
    "df_training = df_training[df_training['price_bucket'].isna() == False]\n",
    "df_training = df_training[df_training['price_bucket'] != '$$$']\n",
    "df_training_X = df_training.drop(['loc_no_address', 'review_rating', 'img1', 'img2', 'img3', 'img4', 'img5', 'price_bucket'],axis=1)\n",
    "df_training_y = df_training['review_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Ubereat_US_Merchant.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_X = df_training.drop(['loc_no_address', 'review_rating', 'img1', 'img2', 'img3', 'img4', 'img5', 'price_bucket'],axis=1)\n",
    "df_training_y = df_training['price_bucket'].apply(convert_price)\n",
    "\n",
    "df_majority = df_training[df_training['price_bucket'] == '$']\n",
    "df_minority = df_training[df_training['price_bucket'] == '$$']\n",
    "\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                    replace=False, \n",
    "                                    n_samples=len(df_minority), \n",
    "                                    random_state=42)\n",
    "\n",
    "downsampled_df = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "df_training_X = downsampled_df.drop(['loc_no_address', 'review_rating', 'img1', 'img2', 'img3', 'img4', 'img5', 'price_bucket'],axis=1)\n",
    "df_training_y = downsampled_df['price_bucket'].apply(convert_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_training_X, df_training_y, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_PCA, X_test_PCA = run_PCA(X_train, X_test, use_all=False, num_components=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(5):\n",
    "   \n",
    "    pca = PCA(n_components=12)\n",
    "    pca.fit(df_training_X[[str(i) for i in range(2048*j, 2048 + 2048*j)]])\n",
    "    with open(f'pca_model{j}.pkl', 'wb') as file:\n",
    "        pickle.dump(pca, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n"
     ]
    }
   ],
   "source": [
    "y_preds_baseline = np.tile(1, y_test.shape[0])\n",
    "baseline_accuracy = y_test[y_test == 1].shape[0]/y_test.shape[0]\n",
    "\n",
    "print(baseline_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n"
     ]
    }
   ],
   "source": [
    "model = SVC(kernel='poly')\n",
    "model.fit(X_train_PCA, y_train)\n",
    "y_preds_SVC = model.predict(X_test_PCA)\n",
    "error_SVC = accuracy_score(y_test, y_preds_SVC)\n",
    "\n",
    "print(error_SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_PCA, y_train)\n",
    "y_preds_LR = model.predict(X_test_PCA)\n",
    "error_LR = accuracy_score(y_test, y_preds_LR)\n",
    "\n",
    "print(error_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-20 20:57:26,991] A new study created in memory with name: no-name-61f0e862-9934-4e5e-bfd1-e53009d9a0da\n",
      "[I 2024-10-20 20:57:27,177] Trial 0 finished with value: 0.6 and parameters: {'n_estimators': 289, 'max_depth': 16, 'min_samples_split': 11, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 0 with value: 0.6.\n",
      "[I 2024-10-20 20:57:27,279] Trial 1 finished with value: 0.68 and parameters: {'n_estimators': 164, 'max_depth': 174, 'min_samples_split': 9, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 1 with value: 0.68.\n",
      "[I 2024-10-20 20:57:27,297] Trial 2 finished with value: 0.6 and parameters: {'n_estimators': 37, 'max_depth': 149, 'min_samples_split': 7, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 1 with value: 0.68.\n",
      "[I 2024-10-20 20:57:27,336] Trial 3 finished with value: 0.68 and parameters: {'n_estimators': 84, 'max_depth': 204, 'min_samples_split': 10, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 1 with value: 0.68.\n",
      "[I 2024-10-20 20:57:27,423] Trial 4 finished with value: 0.68 and parameters: {'n_estimators': 200, 'max_depth': 123, 'min_samples_split': 10, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 1 with value: 0.68.\n",
      "[I 2024-10-20 20:57:27,541] Trial 5 finished with value: 0.6 and parameters: {'n_estimators': 274, 'max_depth': 185, 'min_samples_split': 3, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 1 with value: 0.68.\n",
      "[I 2024-10-20 20:57:27,554] Trial 6 finished with value: 0.64 and parameters: {'n_estimators': 25, 'max_depth': 97, 'min_samples_split': 10, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 1 with value: 0.68.\n",
      "[I 2024-10-20 20:57:27,559] Trial 7 finished with value: 0.6 and parameters: {'n_estimators': 10, 'max_depth': 45, 'min_samples_split': 7, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 1 with value: 0.68.\n",
      "[I 2024-10-20 20:57:27,618] Trial 8 finished with value: 0.68 and parameters: {'n_estimators': 170, 'max_depth': 69, 'min_samples_split': 11, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 1 with value: 0.68.\n",
      "[I 2024-10-20 20:57:27,723] Trial 9 finished with value: 0.72 and parameters: {'n_estimators': 235, 'max_depth': 43, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 9 with value: 0.72.\n",
      "[I 2024-10-20 20:57:27,830] Trial 10 finished with value: 0.64 and parameters: {'n_estimators': 229, 'max_depth': 10, 'min_samples_split': 15, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 9 with value: 0.72.\n",
      "[I 2024-10-20 20:57:27,877] Trial 11 finished with value: 0.64 and parameters: {'n_estimators': 107, 'max_depth': 166, 'min_samples_split': 3, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 9 with value: 0.72.\n",
      "[I 2024-10-20 20:57:27,933] Trial 12 finished with value: 0.6 and parameters: {'n_estimators': 131, 'max_depth': 241, 'min_samples_split': 5, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 9 with value: 0.72.\n",
      "[I 2024-10-20 20:57:28,023] Trial 13 finished with value: 0.68 and parameters: {'n_estimators': 244, 'max_depth': 100, 'min_samples_split': 14, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 9 with value: 0.72.\n",
      "[I 2024-10-20 20:57:28,108] Trial 14 finished with value: 0.68 and parameters: {'n_estimators': 179, 'max_depth': 225, 'min_samples_split': 6, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.72.\n",
      "[I 2024-10-20 20:57:28,195] Trial 15 finished with value: 0.68 and parameters: {'n_estimators': 214, 'max_depth': 134, 'min_samples_split': 2, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 9 with value: 0.72.\n",
      "[I 2024-10-20 20:57:28,257] Trial 16 finished with value: 0.6 and parameters: {'n_estimators': 143, 'max_depth': 65, 'min_samples_split': 8, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 9 with value: 0.72.\n",
      "[I 2024-10-20 20:57:28,379] Trial 17 finished with value: 0.68 and parameters: {'n_estimators': 254, 'max_depth': 174, 'min_samples_split': 5, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 9 with value: 0.72.\n",
      "[I 2024-10-20 20:57:28,472] Trial 18 finished with value: 0.68 and parameters: {'n_estimators': 188, 'max_depth': 204, 'min_samples_split': 13, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 9 with value: 0.72.\n",
      "[I 2024-10-20 20:57:28,508] Trial 19 finished with value: 0.64 and parameters: {'n_estimators': 71, 'max_depth': 106, 'min_samples_split': 4, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 9 with value: 0.72.\n",
      "[I 2024-10-20 20:57:28,573] Trial 20 finished with value: 0.64 and parameters: {'n_estimators': 152, 'max_depth': 45, 'min_samples_split': 8, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 9 with value: 0.72.\n",
      "[I 2024-10-20 20:57:28,625] Trial 21 finished with value: 0.76 and parameters: {'n_estimators': 96, 'max_depth': 202, 'min_samples_split': 9, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:28,678] Trial 22 finished with value: 0.64 and parameters: {'n_estimators': 99, 'max_depth': 199, 'min_samples_split': 9, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:28,742] Trial 23 finished with value: 0.64 and parameters: {'n_estimators': 124, 'max_depth': 153, 'min_samples_split': 12, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:28,777] Trial 24 finished with value: 0.52 and parameters: {'n_estimators': 57, 'max_depth': 224, 'min_samples_split': 6, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:28,857] Trial 25 finished with value: 0.68 and parameters: {'n_estimators': 162, 'max_depth': 181, 'min_samples_split': 9, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:28,957] Trial 26 finished with value: 0.72 and parameters: {'n_estimators': 210, 'max_depth': 245, 'min_samples_split': 7, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:29,078] Trial 27 finished with value: 0.64 and parameters: {'n_estimators': 258, 'max_depth': 245, 'min_samples_split': 7, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:29,183] Trial 28 finished with value: 0.6 and parameters: {'n_estimators': 215, 'max_depth': 228, 'min_samples_split': 5, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:29,319] Trial 29 finished with value: 0.64 and parameters: {'n_estimators': 292, 'max_depth': 31, 'min_samples_split': 4, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:29,431] Trial 30 finished with value: 0.64 and parameters: {'n_estimators': 234, 'max_depth': 75, 'min_samples_split': 2, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:29,527] Trial 31 finished with value: 0.68 and parameters: {'n_estimators': 200, 'max_depth': 218, 'min_samples_split': 11, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:29,631] Trial 32 finished with value: 0.56 and parameters: {'n_estimators': 270, 'max_depth': 193, 'min_samples_split': 8, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:29,693] Trial 33 finished with value: 0.72 and parameters: {'n_estimators': 119, 'max_depth': 156, 'min_samples_split': 7, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:29,749] Trial 34 finished with value: 0.68 and parameters: {'n_estimators': 103, 'max_depth': 142, 'min_samples_split': 6, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:29,783] Trial 35 finished with value: 0.6 and parameters: {'n_estimators': 55, 'max_depth': 165, 'min_samples_split': 7, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:29,846] Trial 36 finished with value: 0.72 and parameters: {'n_estimators': 119, 'max_depth': 120, 'min_samples_split': 10, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:29,892] Trial 37 finished with value: 0.6 and parameters: {'n_estimators': 81, 'max_depth': 209, 'min_samples_split': 8, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:29,966] Trial 38 finished with value: 0.64 and parameters: {'n_estimators': 143, 'max_depth': 237, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:30,071] Trial 39 finished with value: 0.64 and parameters: {'n_estimators': 211, 'max_depth': 151, 'min_samples_split': 7, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:30,202] Trial 40 finished with value: 0.72 and parameters: {'n_estimators': 277, 'max_depth': 88, 'min_samples_split': 9, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:30,265] Trial 41 finished with value: 0.64 and parameters: {'n_estimators': 121, 'max_depth': 189, 'min_samples_split': 10, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:30,324] Trial 42 finished with value: 0.6 and parameters: {'n_estimators': 111, 'max_depth': 112, 'min_samples_split': 10, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:30,374] Trial 43 finished with value: 0.68 and parameters: {'n_estimators': 90, 'max_depth': 118, 'min_samples_split': 11, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:30,446] Trial 44 finished with value: 0.68 and parameters: {'n_estimators': 140, 'max_depth': 24, 'min_samples_split': 12, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:30,534] Trial 45 finished with value: 0.64 and parameters: {'n_estimators': 179, 'max_depth': 128, 'min_samples_split': 9, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:30,577] Trial 46 finished with value: 0.64 and parameters: {'n_estimators': 71, 'max_depth': 164, 'min_samples_split': 6, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:30,641] Trial 47 finished with value: 0.6 and parameters: {'n_estimators': 123, 'max_depth': 249, 'min_samples_split': 10, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:30,671] Trial 48 finished with value: 0.68 and parameters: {'n_estimators': 40, 'max_depth': 59, 'min_samples_split': 3, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:30,778] Trial 49 finished with value: 0.68 and parameters: {'n_estimators': 225, 'max_depth': 85, 'min_samples_split': 7, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:30,830] Trial 50 finished with value: 0.6 and parameters: {'n_estimators': 91, 'max_depth': 211, 'min_samples_split': 5, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:30,961] Trial 51 finished with value: 0.6 and parameters: {'n_estimators': 279, 'max_depth': 90, 'min_samples_split': 9, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:31,083] Trial 52 finished with value: 0.76 and parameters: {'n_estimators': 258, 'max_depth': 137, 'min_samples_split': 12, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:31,199] Trial 53 finished with value: 0.68 and parameters: {'n_estimators': 246, 'max_depth': 133, 'min_samples_split': 12, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:31,323] Trial 54 finished with value: 0.64 and parameters: {'n_estimators': 262, 'max_depth': 147, 'min_samples_split': 13, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:31,418] Trial 55 finished with value: 0.68 and parameters: {'n_estimators': 194, 'max_depth': 180, 'min_samples_split': 15, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:31,531] Trial 56 finished with value: 0.68 and parameters: {'n_estimators': 239, 'max_depth': 232, 'min_samples_split': 11, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:31,590] Trial 57 finished with value: 0.6 and parameters: {'n_estimators': 112, 'max_depth': 158, 'min_samples_split': 14, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:31,681] Trial 58 finished with value: 0.64 and parameters: {'n_estimators': 222, 'max_depth': 142, 'min_samples_split': 8, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:31,760] Trial 59 finished with value: 0.64 and parameters: {'n_estimators': 159, 'max_depth': 123, 'min_samples_split': 13, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:31,878] Trial 60 finished with value: 0.68 and parameters: {'n_estimators': 250, 'max_depth': 171, 'min_samples_split': 10, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:32,018] Trial 61 finished with value: 0.72 and parameters: {'n_estimators': 299, 'max_depth': 100, 'min_samples_split': 9, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:32,150] Trial 62 finished with value: 0.68 and parameters: {'n_estimators': 279, 'max_depth': 115, 'min_samples_split': 8, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:32,276] Trial 63 finished with value: 0.68 and parameters: {'n_estimators': 265, 'max_depth': 54, 'min_samples_split': 9, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:32,416] Trial 64 finished with value: 0.72 and parameters: {'n_estimators': 289, 'max_depth': 38, 'min_samples_split': 6, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:32,531] Trial 65 finished with value: 0.68 and parameters: {'n_estimators': 232, 'max_depth': 72, 'min_samples_split': 10, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:32,600] Trial 66 finished with value: 0.64 and parameters: {'n_estimators': 130, 'max_depth': 140, 'min_samples_split': 12, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:32,674] Trial 67 finished with value: 0.72 and parameters: {'n_estimators': 175, 'max_depth': 81, 'min_samples_split': 11, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:32,804] Trial 68 finished with value: 0.72 and parameters: {'n_estimators': 275, 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:32,882] Trial 69 finished with value: 0.68 and parameters: {'n_estimators': 152, 'max_depth': 107, 'min_samples_split': 8, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:32,998] Trial 70 finished with value: 0.72 and parameters: {'n_estimators': 241, 'max_depth': 96, 'min_samples_split': 3, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:33,139] Trial 71 finished with value: 0.72 and parameters: {'n_estimators': 300, 'max_depth': 99, 'min_samples_split': 9, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:33,273] Trial 72 finished with value: 0.64 and parameters: {'n_estimators': 285, 'max_depth': 126, 'min_samples_split': 9, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:33,413] Trial 73 finished with value: 0.68 and parameters: {'n_estimators': 298, 'max_depth': 109, 'min_samples_split': 9, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:33,542] Trial 74 finished with value: 0.6 and parameters: {'n_estimators': 264, 'max_depth': 93, 'min_samples_split': 10, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:33,664] Trial 75 finished with value: 0.68 and parameters: {'n_estimators': 254, 'max_depth': 134, 'min_samples_split': 8, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:33,709] Trial 76 finished with value: 0.64 and parameters: {'n_estimators': 74, 'max_depth': 197, 'min_samples_split': 5, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:33,796] Trial 77 finished with value: 0.6 and parameters: {'n_estimators': 208, 'max_depth': 118, 'min_samples_split': 7, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:33,865] Trial 78 finished with value: 0.68 and parameters: {'n_estimators': 98, 'max_depth': 105, 'min_samples_split': 10, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:34,163] Trial 79 finished with value: 0.64 and parameters: {'n_estimators': 271, 'max_depth': 158, 'min_samples_split': 6, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:34,249] Trial 80 finished with value: 0.6 and parameters: {'n_estimators': 117, 'max_depth': 78, 'min_samples_split': 8, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:34,396] Trial 81 finished with value: 0.6 and parameters: {'n_estimators': 293, 'max_depth': 38, 'min_samples_split': 6, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:34,535] Trial 82 finished with value: 0.56 and parameters: {'n_estimators': 283, 'max_depth': 45, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:34,672] Trial 83 finished with value: 0.64 and parameters: {'n_estimators': 289, 'max_depth': 22, 'min_samples_split': 7, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:34,797] Trial 84 finished with value: 0.64 and parameters: {'n_estimators': 256, 'max_depth': 56, 'min_samples_split': 5, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:34,868] Trial 85 finished with value: 0.72 and parameters: {'n_estimators': 135, 'max_depth': 86, 'min_samples_split': 9, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:34,997] Trial 86 finished with value: 0.76 and parameters: {'n_estimators': 268, 'max_depth': 64, 'min_samples_split': 8, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:35,127] Trial 87 finished with value: 0.72 and parameters: {'n_estimators': 272, 'max_depth': 65, 'min_samples_split': 8, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:35,243] Trial 88 finished with value: 0.72 and parameters: {'n_estimators': 237, 'max_depth': 68, 'min_samples_split': 9, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:35,315] Trial 89 finished with value: 0.68 and parameters: {'n_estimators': 167, 'max_depth': 218, 'min_samples_split': 12, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:35,436] Trial 90 finished with value: 0.76 and parameters: {'n_estimators': 247, 'max_depth': 239, 'min_samples_split': 11, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:35,551] Trial 91 finished with value: 0.72 and parameters: {'n_estimators': 245, 'max_depth': 240, 'min_samples_split': 10, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:35,662] Trial 92 finished with value: 0.72 and parameters: {'n_estimators': 227, 'max_depth': 250, 'min_samples_split': 11, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:35,769] Trial 93 finished with value: 0.72 and parameters: {'n_estimators': 217, 'max_depth': 232, 'min_samples_split': 11, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:35,890] Trial 94 finished with value: 0.64 and parameters: {'n_estimators': 249, 'max_depth': 216, 'min_samples_split': 11, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:36,015] Trial 95 finished with value: 0.68 and parameters: {'n_estimators': 260, 'max_depth': 241, 'min_samples_split': 10, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:36,075] Trial 96 finished with value: 0.64 and parameters: {'n_estimators': 106, 'max_depth': 122, 'min_samples_split': 8, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:36,208] Trial 97 finished with value: 0.6 and parameters: {'n_estimators': 264, 'max_depth': 206, 'min_samples_split': 12, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:36,342] Trial 98 finished with value: 0.64 and parameters: {'n_estimators': 279, 'max_depth': 223, 'min_samples_split': 9, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:36,471] Trial 99 finished with value: 0.68 and parameters: {'n_estimators': 268, 'max_depth': 49, 'min_samples_split': 2, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.76.\n",
      "[I 2024-10-20 20:57:39,194] A new study created in memory with name: no-name-450850d7-a9f3-4eb8-80d0-fa32fe084070\n",
      "[I 2024-10-20 20:57:39,454] Trial 0 finished with value: 0.52 and parameters: {'n_estimators': 296, 'max_depth': 87, 'min_samples_split': 15, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 0 with value: 0.52.\n",
      "[I 2024-10-20 20:57:39,490] Trial 1 finished with value: 0.4 and parameters: {'n_estimators': 83, 'max_depth': 97, 'min_samples_split': 13, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 0 with value: 0.52.\n",
      "[I 2024-10-20 20:57:39,514] Trial 2 finished with value: 0.6 and parameters: {'n_estimators': 51, 'max_depth': 180, 'min_samples_split': 4, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:39,642] Trial 3 finished with value: 0.48 and parameters: {'n_estimators': 293, 'max_depth': 200, 'min_samples_split': 8, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:39,714] Trial 4 finished with value: 0.44 and parameters: {'n_estimators': 201, 'max_depth': 176, 'min_samples_split': 12, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:39,784] Trial 5 finished with value: 0.44 and parameters: {'n_estimators': 158, 'max_depth': 26, 'min_samples_split': 3, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:39,865] Trial 6 finished with value: 0.6 and parameters: {'n_estimators': 184, 'max_depth': 194, 'min_samples_split': 13, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:39,942] Trial 7 finished with value: 0.56 and parameters: {'n_estimators': 177, 'max_depth': 177, 'min_samples_split': 5, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:39,980] Trial 8 finished with value: 0.48 and parameters: {'n_estimators': 85, 'max_depth': 150, 'min_samples_split': 7, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:40,018] Trial 9 finished with value: 0.52 and parameters: {'n_estimators': 107, 'max_depth': 107, 'min_samples_split': 13, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:40,032] Trial 10 finished with value: 0.4 and parameters: {'n_estimators': 15, 'max_depth': 249, 'min_samples_split': 2, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:40,133] Trial 11 finished with value: 0.44 and parameters: {'n_estimators': 213, 'max_depth': 231, 'min_samples_split': 10, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:40,255] Trial 12 finished with value: 0.52 and parameters: {'n_estimators': 249, 'max_depth': 208, 'min_samples_split': 5, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:40,271] Trial 13 finished with value: 0.48 and parameters: {'n_estimators': 15, 'max_depth': 140, 'min_samples_split': 10, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:40,340] Trial 14 finished with value: 0.48 and parameters: {'n_estimators': 135, 'max_depth': 170, 'min_samples_split': 6, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:40,373] Trial 15 finished with value: 0.48 and parameters: {'n_estimators': 53, 'max_depth': 205, 'min_samples_split': 10, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:40,489] Trial 16 finished with value: 0.52 and parameters: {'n_estimators': 240, 'max_depth': 40, 'min_samples_split': 15, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:40,554] Trial 17 finished with value: 0.44 and parameters: {'n_estimators': 125, 'max_depth': 126, 'min_samples_split': 4, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:40,583] Trial 18 finished with value: 0.44 and parameters: {'n_estimators': 58, 'max_depth': 223, 'min_samples_split': 8, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:40,672] Trial 19 finished with value: 0.48 and parameters: {'n_estimators': 184, 'max_depth': 63, 'min_samples_split': 12, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:40,747] Trial 20 finished with value: 0.48 and parameters: {'n_estimators': 146, 'max_depth': 159, 'min_samples_split': 2, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:40,831] Trial 21 finished with value: 0.44 and parameters: {'n_estimators': 173, 'max_depth': 178, 'min_samples_split': 5, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:40,937] Trial 22 finished with value: 0.48 and parameters: {'n_estimators': 222, 'max_depth': 189, 'min_samples_split': 4, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:41,023] Trial 23 finished with value: 0.48 and parameters: {'n_estimators': 179, 'max_depth': 136, 'min_samples_split': 6, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:41,082] Trial 24 finished with value: 0.4 and parameters: {'n_estimators': 113, 'max_depth': 227, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:41,202] Trial 25 finished with value: 0.52 and parameters: {'n_estimators': 258, 'max_depth': 189, 'min_samples_split': 9, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:41,269] Trial 26 finished with value: 0.44 and parameters: {'n_estimators': 159, 'max_depth': 157, 'min_samples_split': 7, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:41,364] Trial 27 finished with value: 0.44 and parameters: {'n_estimators': 198, 'max_depth': 122, 'min_samples_split': 3, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:41,392] Trial 28 finished with value: 0.44 and parameters: {'n_estimators': 41, 'max_depth': 215, 'min_samples_split': 5, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:41,519] Trial 29 finished with value: 0.44 and parameters: {'n_estimators': 282, 'max_depth': 244, 'min_samples_split': 14, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:41,568] Trial 30 finished with value: 0.44 and parameters: {'n_estimators': 91, 'max_depth': 191, 'min_samples_split': 11, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:41,689] Trial 31 finished with value: 0.52 and parameters: {'n_estimators': 265, 'max_depth': 79, 'min_samples_split': 15, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:41,825] Trial 32 finished with value: 0.48 and parameters: {'n_estimators': 300, 'max_depth': 94, 'min_samples_split': 14, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:41,914] Trial 33 finished with value: 0.44 and parameters: {'n_estimators': 229, 'max_depth': 103, 'min_samples_split': 14, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:42,039] Trial 34 finished with value: 0.6 and parameters: {'n_estimators': 274, 'max_depth': 60, 'min_samples_split': 13, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:42,132] Trial 35 finished with value: 0.52 and parameters: {'n_estimators': 196, 'max_depth': 13, 'min_samples_split': 13, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:42,274] Trial 36 finished with value: 0.52 and parameters: {'n_estimators': 277, 'max_depth': 49, 'min_samples_split': 12, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:42,361] Trial 37 finished with value: 0.52 and parameters: {'n_estimators': 166, 'max_depth': 168, 'min_samples_split': 7, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:42,450] Trial 38 finished with value: 0.44 and parameters: {'n_estimators': 215, 'max_depth': 113, 'min_samples_split': 9, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:42,527] Trial 39 finished with value: 0.48 and parameters: {'n_estimators': 143, 'max_depth': 149, 'min_samples_split': 11, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:42,577] Trial 40 finished with value: 0.44 and parameters: {'n_estimators': 99, 'max_depth': 77, 'min_samples_split': 3, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:42,718] Trial 41 finished with value: 0.52 and parameters: {'n_estimators': 288, 'max_depth': 87, 'min_samples_split': 13, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:42,846] Trial 42 finished with value: 0.6 and parameters: {'n_estimators': 271, 'max_depth': 60, 'min_samples_split': 15, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:42,959] Trial 43 finished with value: 0.6 and parameters: {'n_estimators': 239, 'max_depth': 58, 'min_samples_split': 15, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:43,078] Trial 44 finished with value: 0.48 and parameters: {'n_estimators': 239, 'max_depth': 44, 'min_samples_split': 15, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:43,208] Trial 45 finished with value: 0.48 and parameters: {'n_estimators': 267, 'max_depth': 62, 'min_samples_split': 14, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:43,326] Trial 46 finished with value: 0.44 and parameters: {'n_estimators': 249, 'max_depth': 56, 'min_samples_split': 13, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:43,433] Trial 47 finished with value: 0.52 and parameters: {'n_estimators': 229, 'max_depth': 74, 'min_samples_split': 15, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:43,562] Trial 48 finished with value: 0.52 and parameters: {'n_estimators': 275, 'max_depth': 31, 'min_samples_split': 11, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:43,680] Trial 49 finished with value: 0.52 and parameters: {'n_estimators': 253, 'max_depth': 28, 'min_samples_split': 14, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:43,779] Trial 50 finished with value: 0.48 and parameters: {'n_estimators': 208, 'max_depth': 70, 'min_samples_split': 12, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:43,874] Trial 51 finished with value: 0.48 and parameters: {'n_estimators': 192, 'max_depth': 199, 'min_samples_split': 15, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:44,014] Trial 52 finished with value: 0.56 and parameters: {'n_estimators': 290, 'max_depth': 10, 'min_samples_split': 13, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:44,126] Trial 53 finished with value: 0.52 and parameters: {'n_estimators': 237, 'max_depth': 182, 'min_samples_split': 15, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:44,214] Trial 54 finished with value: 0.56 and parameters: {'n_estimators': 169, 'max_depth': 171, 'min_samples_split': 6, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:44,290] Trial 55 finished with value: 0.48 and parameters: {'n_estimators': 131, 'max_depth': 37, 'min_samples_split': 4, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:44,374] Trial 56 finished with value: 0.44 and parameters: {'n_estimators': 187, 'max_depth': 200, 'min_samples_split': 3, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:44,403] Trial 57 finished with value: 0.56 and parameters: {'n_estimators': 35, 'max_depth': 215, 'min_samples_split': 14, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:44,488] Trial 58 finished with value: 0.44 and parameters: {'n_estimators': 153, 'max_depth': 54, 'min_samples_split': 8, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:44,621] Trial 59 finished with value: 0.56 and parameters: {'n_estimators': 265, 'max_depth': 237, 'min_samples_split': 12, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:44,683] Trial 60 finished with value: 0.48 and parameters: {'n_estimators': 117, 'max_depth': 142, 'min_samples_split': 2, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:44,825] Trial 61 finished with value: 0.48 and parameters: {'n_estimators': 293, 'max_depth': 13, 'min_samples_split': 13, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:44,874] Trial 62 finished with value: 0.48 and parameters: {'n_estimators': 78, 'max_depth': 116, 'min_samples_split': 12, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:45,096] Trial 63 finished with value: 0.48 and parameters: {'n_estimators': 279, 'max_depth': 162, 'min_samples_split': 5, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:45,370] Trial 64 finished with value: 0.44 and parameters: {'n_estimators': 286, 'max_depth': 22, 'min_samples_split': 14, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:45,538] Trial 65 finished with value: 0.48 and parameters: {'n_estimators': 300, 'max_depth': 92, 'min_samples_split': 13, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:45,666] Trial 66 finished with value: 0.44 and parameters: {'n_estimators': 271, 'max_depth': 65, 'min_samples_split': 11, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:45,788] Trial 67 finished with value: 0.56 and parameters: {'n_estimators': 256, 'max_depth': 86, 'min_samples_split': 10, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:45,875] Trial 68 finished with value: 0.44 and parameters: {'n_estimators': 207, 'max_depth': 20, 'min_samples_split': 15, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:45,969] Trial 69 finished with value: 0.6 and parameters: {'n_estimators': 180, 'max_depth': 101, 'min_samples_split': 13, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:46,066] Trial 70 finished with value: 0.48 and parameters: {'n_estimators': 184, 'max_depth': 102, 'min_samples_split': 5, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:46,146] Trial 71 finished with value: 0.52 and parameters: {'n_estimators': 157, 'max_depth': 34, 'min_samples_split': 13, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:46,238] Trial 72 finished with value: 0.48 and parameters: {'n_estimators': 176, 'max_depth': 49, 'min_samples_split': 14, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:46,371] Trial 73 finished with value: 0.52 and parameters: {'n_estimators': 249, 'max_depth': 134, 'min_samples_split': 13, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:46,451] Trial 74 finished with value: 0.52 and parameters: {'n_estimators': 142, 'max_depth': 183, 'min_samples_split': 12, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:46,566] Trial 75 finished with value: 0.52 and parameters: {'n_estimators': 217, 'max_depth': 192, 'min_samples_split': 4, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:46,658] Trial 76 finished with value: 0.48 and parameters: {'n_estimators': 163, 'max_depth': 42, 'min_samples_split': 6, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:46,680] Trial 77 finished with value: 0.52 and parameters: {'n_estimators': 11, 'max_depth': 153, 'min_samples_split': 14, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:46,794] Trial 78 finished with value: 0.48 and parameters: {'n_estimators': 260, 'max_depth': 206, 'min_samples_split': 15, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:46,941] Trial 79 finished with value: 0.44 and parameters: {'n_estimators': 292, 'max_depth': 80, 'min_samples_split': 13, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:46,984] Trial 80 finished with value: 0.48 and parameters: {'n_estimators': 65, 'max_depth': 109, 'min_samples_split': 14, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:47,068] Trial 81 finished with value: 0.52 and parameters: {'n_estimators': 170, 'max_depth': 174, 'min_samples_split': 6, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:47,147] Trial 82 finished with value: 0.48 and parameters: {'n_estimators': 150, 'max_depth': 123, 'min_samples_split': 7, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:47,245] Trial 83 finished with value: 0.6 and parameters: {'n_estimators': 201, 'max_depth': 166, 'min_samples_split': 6, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:47,344] Trial 84 finished with value: 0.52 and parameters: {'n_estimators': 205, 'max_depth': 182, 'min_samples_split': 5, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:47,455] Trial 85 finished with value: 0.52 and parameters: {'n_estimators': 233, 'max_depth': 165, 'min_samples_split': 4, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:47,562] Trial 86 finished with value: 0.52 and parameters: {'n_estimators': 224, 'max_depth': 215, 'min_samples_split': 3, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:47,660] Trial 87 finished with value: 0.48 and parameters: {'n_estimators': 197, 'max_depth': 147, 'min_samples_split': 12, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:47,775] Trial 88 finished with value: 0.48 and parameters: {'n_estimators': 244, 'max_depth': 156, 'min_samples_split': 9, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:47,865] Trial 89 finished with value: 0.44 and parameters: {'n_estimators': 182, 'max_depth': 59, 'min_samples_split': 11, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:47,944] Trial 90 finished with value: 0.4 and parameters: {'n_estimators': 194, 'max_depth': 68, 'min_samples_split': 15, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:48,029] Trial 91 finished with value: 0.44 and parameters: {'n_estimators': 167, 'max_depth': 171, 'min_samples_split': 6, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:48,117] Trial 92 finished with value: 0.52 and parameters: {'n_estimators': 177, 'max_depth': 195, 'min_samples_split': 5, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:48,211] Trial 93 finished with value: 0.52 and parameters: {'n_estimators': 188, 'max_depth': 178, 'min_samples_split': 6, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:48,345] Trial 94 finished with value: 0.52 and parameters: {'n_estimators': 282, 'max_depth': 186, 'min_samples_split': 8, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:48,436] Trial 95 finished with value: 0.52 and parameters: {'n_estimators': 172, 'max_depth': 167, 'min_samples_split': 7, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:48,537] Trial 96 finished with value: 0.52 and parameters: {'n_estimators': 202, 'max_depth': 47, 'min_samples_split': 6, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:48,619] Trial 97 finished with value: 0.52 and parameters: {'n_estimators': 162, 'max_depth': 175, 'min_samples_split': 4, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:48,724] Trial 98 finished with value: 0.48 and parameters: {'n_estimators': 211, 'max_depth': 197, 'min_samples_split': 5, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:48,854] Trial 99 finished with value: 0.52 and parameters: {'n_estimators': 271, 'max_depth': 161, 'min_samples_split': 13, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 2 with value: 0.6.\n",
      "[I 2024-10-20 20:57:51,614] A new study created in memory with name: no-name-95f8d979-4875-4163-8935-728b02efe1e3\n",
      "[I 2024-10-20 20:57:51,679] Trial 0 finished with value: 0.72 and parameters: {'n_estimators': 127, 'max_depth': 75, 'min_samples_split': 4, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 0 with value: 0.72.\n",
      "[I 2024-10-20 20:57:51,756] Trial 1 finished with value: 0.68 and parameters: {'n_estimators': 103, 'max_depth': 219, 'min_samples_split': 5, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 0 with value: 0.72.\n",
      "[I 2024-10-20 20:57:51,897] Trial 2 finished with value: 0.64 and parameters: {'n_estimators': 209, 'max_depth': 29, 'min_samples_split': 6, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 0 with value: 0.72.\n",
      "[I 2024-10-20 20:57:51,966] Trial 3 finished with value: 0.68 and parameters: {'n_estimators': 198, 'max_depth': 15, 'min_samples_split': 7, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 0 with value: 0.72.\n",
      "[I 2024-10-20 20:57:52,005] Trial 4 finished with value: 0.64 and parameters: {'n_estimators': 100, 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 0 with value: 0.72.\n",
      "[I 2024-10-20 20:57:52,101] Trial 5 finished with value: 0.68 and parameters: {'n_estimators': 276, 'max_depth': 46, 'min_samples_split': 6, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 0 with value: 0.72.\n",
      "[I 2024-10-20 20:57:52,171] Trial 6 finished with value: 0.72 and parameters: {'n_estimators': 147, 'max_depth': 85, 'min_samples_split': 6, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 0 with value: 0.72.\n",
      "[I 2024-10-20 20:57:52,223] Trial 7 finished with value: 0.64 and parameters: {'n_estimators': 145, 'max_depth': 176, 'min_samples_split': 2, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 0 with value: 0.72.\n",
      "[I 2024-10-20 20:57:52,246] Trial 8 finished with value: 0.6 and parameters: {'n_estimators': 63, 'max_depth': 149, 'min_samples_split': 11, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 0 with value: 0.72.\n",
      "[I 2024-10-20 20:57:52,346] Trial 9 finished with value: 0.76 and parameters: {'n_estimators': 222, 'max_depth': 127, 'min_samples_split': 3, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:52,474] Trial 10 finished with value: 0.64 and parameters: {'n_estimators': 271, 'max_depth': 115, 'min_samples_split': 15, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:52,495] Trial 11 finished with value: 0.52 and parameters: {'n_estimators': 23, 'max_depth': 85, 'min_samples_split': 2, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:52,599] Trial 12 finished with value: 0.76 and parameters: {'n_estimators': 224, 'max_depth': 86, 'min_samples_split': 3, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:52,698] Trial 13 finished with value: 0.72 and parameters: {'n_estimators': 214, 'max_depth': 140, 'min_samples_split': 10, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:52,803] Trial 14 finished with value: 0.72 and parameters: {'n_estimators': 231, 'max_depth': 190, 'min_samples_split': 3, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:52,914] Trial 15 finished with value: 0.72 and parameters: {'n_estimators': 241, 'max_depth': 117, 'min_samples_split': 10, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:52,999] Trial 16 finished with value: 0.76 and parameters: {'n_estimators': 181, 'max_depth': 63, 'min_samples_split': 8, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:53,130] Trial 17 finished with value: 0.6 and parameters: {'n_estimators': 290, 'max_depth': 103, 'min_samples_split': 15, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:53,213] Trial 18 finished with value: 0.76 and parameters: {'n_estimators': 176, 'max_depth': 250, 'min_samples_split': 4, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:53,327] Trial 19 finished with value: 0.68 and parameters: {'n_estimators': 246, 'max_depth': 163, 'min_samples_split': 13, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:53,453] Trial 20 finished with value: 0.72 and parameters: {'n_estimators': 260, 'max_depth': 55, 'min_samples_split': 3, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:53,553] Trial 21 finished with value: 0.76 and parameters: {'n_estimators': 177, 'max_depth': 61, 'min_samples_split': 8, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:53,652] Trial 22 finished with value: 0.68 and parameters: {'n_estimators': 180, 'max_depth': 102, 'min_samples_split': 9, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:53,757] Trial 23 finished with value: 0.68 and parameters: {'n_estimators': 222, 'max_depth': 75, 'min_samples_split': 13, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:53,844] Trial 24 finished with value: 0.68 and parameters: {'n_estimators': 186, 'max_depth': 129, 'min_samples_split': 8, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:53,924] Trial 25 finished with value: 0.68 and parameters: {'n_estimators': 161, 'max_depth': 41, 'min_samples_split': 4, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:54,060] Trial 26 finished with value: 0.64 and parameters: {'n_estimators': 296, 'max_depth': 101, 'min_samples_split': 2, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:54,178] Trial 27 finished with value: 0.72 and parameters: {'n_estimators': 248, 'max_depth': 67, 'min_samples_split': 7, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:54,277] Trial 28 finished with value: 0.68 and parameters: {'n_estimators': 201, 'max_depth': 126, 'min_samples_split': 3, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:54,342] Trial 29 finished with value: 0.76 and parameters: {'n_estimators': 130, 'max_depth': 84, 'min_samples_split': 5, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:54,403] Trial 30 finished with value: 0.6 and parameters: {'n_estimators': 119, 'max_depth': 150, 'min_samples_split': 12, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:54,484] Trial 31 finished with value: 0.68 and parameters: {'n_estimators': 163, 'max_depth': 245, 'min_samples_split': 4, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:54,589] Trial 32 finished with value: 0.72 and parameters: {'n_estimators': 225, 'max_depth': 196, 'min_samples_split': 4, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:54,679] Trial 33 finished with value: 0.64 and parameters: {'n_estimators': 188, 'max_depth': 247, 'min_samples_split': 3, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:54,775] Trial 34 finished with value: 0.72 and parameters: {'n_estimators': 206, 'max_depth': 212, 'min_samples_split': 4, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:54,845] Trial 35 finished with value: 0.68 and parameters: {'n_estimators': 170, 'max_depth': 222, 'min_samples_split': 5, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:54,893] Trial 36 finished with value: 0.68 and parameters: {'n_estimators': 89, 'max_depth': 43, 'min_samples_split': 6, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:54,972] Trial 37 finished with value: 0.68 and parameters: {'n_estimators': 195, 'max_depth': 28, 'min_samples_split': 5, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:55,040] Trial 38 finished with value: 0.64 and parameters: {'n_estimators': 136, 'max_depth': 91, 'min_samples_split': 7, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:55,129] Trial 39 finished with value: 0.68 and parameters: {'n_estimators': 236, 'max_depth': 28, 'min_samples_split': 2, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:55,208] Trial 40 finished with value: 0.64 and parameters: {'n_estimators': 154, 'max_depth': 142, 'min_samples_split': 6, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:55,309] Trial 41 finished with value: 0.68 and parameters: {'n_estimators': 214, 'max_depth': 66, 'min_samples_split': 8, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:55,393] Trial 42 finished with value: 0.72 and parameters: {'n_estimators': 175, 'max_depth': 57, 'min_samples_split': 9, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:55,559] Trial 43 finished with value: 0.68 and parameters: {'n_estimators': 113, 'max_depth': 73, 'min_samples_split': 8, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:55,739] Trial 44 finished with value: 0.72 and parameters: {'n_estimators': 195, 'max_depth': 56, 'min_samples_split': 3, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:55,809] Trial 45 finished with value: 0.72 and parameters: {'n_estimators': 145, 'max_depth': 167, 'min_samples_split': 10, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:55,926] Trial 46 finished with value: 0.72 and parameters: {'n_estimators': 216, 'max_depth': 113, 'min_samples_split': 2, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:56,059] Trial 47 finished with value: 0.76 and parameters: {'n_estimators': 256, 'max_depth': 92, 'min_samples_split': 6, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:56,154] Trial 48 finished with value: 0.68 and parameters: {'n_estimators': 174, 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:56,178] Trial 49 finished with value: 0.56 and parameters: {'n_estimators': 18, 'max_depth': 37, 'min_samples_split': 5, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:56,302] Trial 50 finished with value: 0.68 and parameters: {'n_estimators': 281, 'max_depth': 112, 'min_samples_split': 11, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:56,379] Trial 51 finished with value: 0.64 and parameters: {'n_estimators': 138, 'max_depth': 83, 'min_samples_split': 3, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:56,463] Trial 52 finished with value: 0.76 and parameters: {'n_estimators': 128, 'max_depth': 81, 'min_samples_split': 5, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:56,523] Trial 53 finished with value: 0.76 and parameters: {'n_estimators': 101, 'max_depth': 65, 'min_samples_split': 4, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:56,602] Trial 54 finished with value: 0.68 and parameters: {'n_estimators': 156, 'max_depth': 93, 'min_samples_split': 9, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:56,694] Trial 55 finished with value: 0.68 and parameters: {'n_estimators': 185, 'max_depth': 57, 'min_samples_split': 5, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:56,805] Trial 56 finished with value: 0.72 and parameters: {'n_estimators': 231, 'max_depth': 124, 'min_samples_split': 2, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:56,850] Trial 57 finished with value: 0.68 and parameters: {'n_estimators': 78, 'max_depth': 73, 'min_samples_split': 3, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:56,949] Trial 58 finished with value: 0.64 and parameters: {'n_estimators': 207, 'max_depth': 50, 'min_samples_split': 7, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:57,031] Trial 59 finished with value: 0.6 and parameters: {'n_estimators': 168, 'max_depth': 105, 'min_samples_split': 4, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:57,107] Trial 60 finished with value: 0.72 and parameters: {'n_estimators': 152, 'max_depth': 140, 'min_samples_split': 6, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:57,228] Trial 61 finished with value: 0.76 and parameters: {'n_estimators': 262, 'max_depth': 90, 'min_samples_split': 8, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:57,345] Trial 62 finished with value: 0.76 and parameters: {'n_estimators': 250, 'max_depth': 93, 'min_samples_split': 6, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:57,464] Trial 63 finished with value: 0.76 and parameters: {'n_estimators': 257, 'max_depth': 79, 'min_samples_split': 4, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:57,494] Trial 64 finished with value: 0.6 and parameters: {'n_estimators': 42, 'max_depth': 101, 'min_samples_split': 7, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:57,617] Trial 65 finished with value: 0.72 and parameters: {'n_estimators': 265, 'max_depth': 65, 'min_samples_split': 5, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:57,710] Trial 66 finished with value: 0.76 and parameters: {'n_estimators': 192, 'max_depth': 183, 'min_samples_split': 6, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:57,839] Trial 67 finished with value: 0.72 and parameters: {'n_estimators': 275, 'max_depth': 118, 'min_samples_split': 3, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:57,927] Trial 68 finished with value: 0.72 and parameters: {'n_estimators': 223, 'max_depth': 157, 'min_samples_split': 4, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:58,027] Trial 69 finished with value: 0.68 and parameters: {'n_estimators': 203, 'max_depth': 135, 'min_samples_split': 3, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:58,140] Trial 70 finished with value: 0.68 and parameters: {'n_estimators': 237, 'max_depth': 36, 'min_samples_split': 9, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:58,210] Trial 71 finished with value: 0.6 and parameters: {'n_estimators': 130, 'max_depth': 82, 'min_samples_split': 5, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:58,281] Trial 72 finished with value: 0.72 and parameters: {'n_estimators': 125, 'max_depth': 73, 'min_samples_split': 5, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:58,346] Trial 73 finished with value: 0.72 and parameters: {'n_estimators': 113, 'max_depth': 108, 'min_samples_split': 6, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:58,423] Trial 74 finished with value: 0.72 and parameters: {'n_estimators': 139, 'max_depth': 87, 'min_samples_split': 8, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:58,522] Trial 75 finished with value: 0.68 and parameters: {'n_estimators': 179, 'max_depth': 97, 'min_samples_split': 5, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:58,604] Trial 76 finished with value: 0.72 and parameters: {'n_estimators': 165, 'max_depth': 237, 'min_samples_split': 4, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:58,665] Trial 77 finished with value: 0.6 and parameters: {'n_estimators': 108, 'max_depth': 61, 'min_samples_split': 7, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:58,770] Trial 78 finished with value: 0.76 and parameters: {'n_estimators': 214, 'max_depth': 49, 'min_samples_split': 10, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:58,824] Trial 79 finished with value: 0.68 and parameters: {'n_estimators': 121, 'max_depth': 78, 'min_samples_split': 2, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:58,878] Trial 80 finished with value: 0.68 and parameters: {'n_estimators': 91, 'max_depth': 209, 'min_samples_split': 4, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:58,924] Trial 81 finished with value: 0.76 and parameters: {'n_estimators': 76, 'max_depth': 71, 'min_samples_split': 4, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:58,980] Trial 82 finished with value: 0.68 and parameters: {'n_estimators': 101, 'max_depth': 63, 'min_samples_split': 3, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:59,054] Trial 83 finished with value: 0.72 and parameters: {'n_estimators': 143, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:59,124] Trial 84 finished with value: 0.76 and parameters: {'n_estimators': 134, 'max_depth': 51, 'min_samples_split': 5, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:59,255] Trial 85 finished with value: 0.76 and parameters: {'n_estimators': 284, 'max_depth': 85, 'min_samples_split': 6, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:59,332] Trial 86 finished with value: 0.72 and parameters: {'n_estimators': 149, 'max_depth': 78, 'min_samples_split': 14, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:59,446] Trial 87 finished with value: 0.76 and parameters: {'n_estimators': 243, 'max_depth': 40, 'min_samples_split': 5, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:59,525] Trial 88 finished with value: 0.68 and parameters: {'n_estimators': 157, 'max_depth': 70, 'min_samples_split': 4, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:59,579] Trial 89 finished with value: 0.56 and parameters: {'n_estimators': 95, 'max_depth': 98, 'min_samples_split': 3, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:59,670] Trial 90 finished with value: 0.76 and parameters: {'n_estimators': 181, 'max_depth': 123, 'min_samples_split': 8, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:59,796] Trial 91 finished with value: 0.72 and parameters: {'n_estimators': 255, 'max_depth': 91, 'min_samples_split': 8, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:57:59,941] Trial 92 finished with value: 0.76 and parameters: {'n_estimators': 298, 'max_depth': 60, 'min_samples_split': 9, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:58:00,064] Trial 93 finished with value: 0.68 and parameters: {'n_estimators': 261, 'max_depth': 87, 'min_samples_split': 7, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:58:00,192] Trial 94 finished with value: 0.72 and parameters: {'n_estimators': 267, 'max_depth': 108, 'min_samples_split': 8, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:58:00,302] Trial 95 finished with value: 0.64 and parameters: {'n_estimators': 229, 'max_depth': 67, 'min_samples_split': 3, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:58:00,376] Trial 96 finished with value: 0.64 and parameters: {'n_estimators': 173, 'max_depth': 96, 'min_samples_split': 7, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:58:00,482] Trial 97 finished with value: 0.72 and parameters: {'n_estimators': 199, 'max_depth': 80, 'min_samples_split': 5, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:58:00,587] Trial 98 finished with value: 0.76 and parameters: {'n_estimators': 162, 'max_depth': 150, 'min_samples_split': 10, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:58:00,694] Trial 99 finished with value: 0.72 and parameters: {'n_estimators': 190, 'max_depth': 169, 'min_samples_split': 9, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 9 with value: 0.76.\n",
      "[I 2024-10-20 20:58:03,508] A new study created in memory with name: no-name-184042ad-0293-43ed-a63c-78c4d08693e4\n",
      "[I 2024-10-20 20:58:03,540] Trial 0 finished with value: 0.52 and parameters: {'n_estimators': 24, 'max_depth': 194, 'min_samples_split': 12, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 0 with value: 0.52.\n",
      "[I 2024-10-20 20:58:03,695] Trial 1 finished with value: 0.6 and parameters: {'n_estimators': 197, 'max_depth': 241, 'min_samples_split': 6, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:03,821] Trial 2 finished with value: 0.52 and parameters: {'n_estimators': 261, 'max_depth': 33, 'min_samples_split': 10, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:03,888] Trial 3 finished with value: 0.6 and parameters: {'n_estimators': 186, 'max_depth': 183, 'min_samples_split': 5, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:03,900] Trial 4 finished with value: 0.52 and parameters: {'n_estimators': 32, 'max_depth': 150, 'min_samples_split': 9, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:03,985] Trial 5 finished with value: 0.56 and parameters: {'n_estimators': 247, 'max_depth': 155, 'min_samples_split': 7, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:04,073] Trial 6 finished with value: 0.52 and parameters: {'n_estimators': 204, 'max_depth': 136, 'min_samples_split': 2, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:04,178] Trial 7 finished with value: 0.48 and parameters: {'n_estimators': 239, 'max_depth': 58, 'min_samples_split': 15, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:04,220] Trial 8 finished with value: 0.52 and parameters: {'n_estimators': 94, 'max_depth': 182, 'min_samples_split': 13, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:04,286] Trial 9 finished with value: 0.6 and parameters: {'n_estimators': 179, 'max_depth': 226, 'min_samples_split': 11, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:04,338] Trial 10 finished with value: 0.56 and parameters: {'n_estimators': 120, 'max_depth': 245, 'min_samples_split': 5, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:04,402] Trial 11 finished with value: 0.56 and parameters: {'n_estimators': 146, 'max_depth': 93, 'min_samples_split': 5, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:04,483] Trial 12 finished with value: 0.52 and parameters: {'n_estimators': 206, 'max_depth': 208, 'min_samples_split': 2, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:04,592] Trial 13 finished with value: 0.56 and parameters: {'n_estimators': 284, 'max_depth': 250, 'min_samples_split': 6, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:04,660] Trial 14 finished with value: 0.52 and parameters: {'n_estimators': 172, 'max_depth': 179, 'min_samples_split': 4, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:04,694] Trial 15 finished with value: 0.56 and parameters: {'n_estimators': 73, 'max_depth': 112, 'min_samples_split': 8, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:04,776] Trial 16 finished with value: 0.56 and parameters: {'n_estimators': 213, 'max_depth': 219, 'min_samples_split': 4, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:04,835] Trial 17 finished with value: 0.52 and parameters: {'n_estimators': 142, 'max_depth': 175, 'min_samples_split': 7, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:04,986] Trial 18 finished with value: 0.6 and parameters: {'n_estimators': 184, 'max_depth': 223, 'min_samples_split': 3, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:05,111] Trial 19 finished with value: 0.56 and parameters: {'n_estimators': 115, 'max_depth': 89, 'min_samples_split': 8, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:05,254] Trial 20 finished with value: 0.56 and parameters: {'n_estimators': 294, 'max_depth': 199, 'min_samples_split': 6, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 1 with value: 0.6.\n",
      "[I 2024-10-20 20:58:05,329] Trial 21 finished with value: 0.64 and parameters: {'n_estimators': 160, 'max_depth': 232, 'min_samples_split': 10, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:05,421] Trial 22 finished with value: 0.56 and parameters: {'n_estimators': 224, 'max_depth': 235, 'min_samples_split': 10, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:05,485] Trial 23 finished with value: 0.56 and parameters: {'n_estimators': 157, 'max_depth': 206, 'min_samples_split': 9, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:05,538] Trial 24 finished with value: 0.6 and parameters: {'n_estimators': 124, 'max_depth': 164, 'min_samples_split': 7, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:05,613] Trial 25 finished with value: 0.56 and parameters: {'n_estimators': 193, 'max_depth': 248, 'min_samples_split': 14, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:05,691] Trial 26 finished with value: 0.56 and parameters: {'n_estimators': 156, 'max_depth': 219, 'min_samples_split': 5, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:05,778] Trial 27 finished with value: 0.56 and parameters: {'n_estimators': 229, 'max_depth': 193, 'min_samples_split': 10, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:05,821] Trial 28 finished with value: 0.56 and parameters: {'n_estimators': 92, 'max_depth': 233, 'min_samples_split': 6, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:05,942] Trial 29 finished with value: 0.52 and parameters: {'n_estimators': 257, 'max_depth': 196, 'min_samples_split': 12, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:06,013] Trial 30 finished with value: 0.6 and parameters: {'n_estimators': 172, 'max_depth': 133, 'min_samples_split': 4, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:06,085] Trial 31 finished with value: 0.56 and parameters: {'n_estimators': 181, 'max_depth': 230, 'min_samples_split': 12, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:06,161] Trial 32 finished with value: 0.6 and parameters: {'n_estimators': 195, 'max_depth': 213, 'min_samples_split': 10, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:06,218] Trial 33 finished with value: 0.56 and parameters: {'n_estimators': 135, 'max_depth': 14, 'min_samples_split': 11, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:06,284] Trial 34 finished with value: 0.56 and parameters: {'n_estimators': 164, 'max_depth': 235, 'min_samples_split': 9, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:06,368] Trial 35 finished with value: 0.6 and parameters: {'n_estimators': 215, 'max_depth': 162, 'min_samples_split': 11, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:06,471] Trial 36 finished with value: 0.6 and parameters: {'n_estimators': 266, 'max_depth': 187, 'min_samples_split': 13, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:06,496] Trial 37 finished with value: 0.56 and parameters: {'n_estimators': 34, 'max_depth': 214, 'min_samples_split': 8, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:06,572] Trial 38 finished with value: 0.56 and parameters: {'n_estimators': 191, 'max_depth': 146, 'min_samples_split': 11, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:06,664] Trial 39 finished with value: 0.52 and parameters: {'n_estimators': 236, 'max_depth': 238, 'min_samples_split': 7, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:06,753] Trial 40 finished with value: 0.56 and parameters: {'n_estimators': 172, 'max_depth': 226, 'min_samples_split': 13, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:06,849] Trial 41 finished with value: 0.56 and parameters: {'n_estimators': 192, 'max_depth': 201, 'min_samples_split': 3, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:06,936] Trial 42 finished with value: 0.56 and parameters: {'n_estimators': 181, 'max_depth': 224, 'min_samples_split': 3, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:07,033] Trial 43 finished with value: 0.56 and parameters: {'n_estimators': 207, 'max_depth': 249, 'min_samples_split': 3, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:07,107] Trial 44 finished with value: 0.6 and parameters: {'n_estimators': 146, 'max_depth': 172, 'min_samples_split': 2, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:07,174] Trial 45 finished with value: 0.56 and parameters: {'n_estimators': 130, 'max_depth': 240, 'min_samples_split': 5, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:07,248] Trial 46 finished with value: 0.56 and parameters: {'n_estimators': 180, 'max_depth': 210, 'min_samples_split': 4, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:07,353] Trial 47 finished with value: 0.52 and parameters: {'n_estimators': 217, 'max_depth': 186, 'min_samples_split': 6, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:07,448] Trial 48 finished with value: 0.6 and parameters: {'n_estimators': 249, 'max_depth': 223, 'min_samples_split': 5, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:07,498] Trial 49 finished with value: 0.6 and parameters: {'n_estimators': 111, 'max_depth': 241, 'min_samples_split': 3, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:07,566] Trial 50 finished with value: 0.52 and parameters: {'n_estimators': 162, 'max_depth': 110, 'min_samples_split': 9, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:07,610] Trial 51 finished with value: 0.56 and parameters: {'n_estimators': 97, 'max_depth': 165, 'min_samples_split': 6, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:07,666] Trial 52 finished with value: 0.56 and parameters: {'n_estimators': 130, 'max_depth': 150, 'min_samples_split': 7, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:07,728] Trial 53 finished with value: 0.52 and parameters: {'n_estimators': 148, 'max_depth': 203, 'min_samples_split': 7, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:07,799] Trial 54 finished with value: 0.56 and parameters: {'n_estimators': 169, 'max_depth': 122, 'min_samples_split': 8, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:07,882] Trial 55 finished with value: 0.52 and parameters: {'n_estimators': 200, 'max_depth': 177, 'min_samples_split': 2, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:07,960] Trial 56 finished with value: 0.56 and parameters: {'n_estimators': 182, 'max_depth': 142, 'min_samples_split': 4, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:08,016] Trial 57 finished with value: 0.52 and parameters: {'n_estimators': 119, 'max_depth': 70, 'min_samples_split': 10, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:08,052] Trial 58 finished with value: 0.56 and parameters: {'n_estimators': 64, 'max_depth': 192, 'min_samples_split': 7, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:08,132] Trial 59 finished with value: 0.48 and parameters: {'n_estimators': 151, 'max_depth': 164, 'min_samples_split': 6, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:08,179] Trial 60 finished with value: 0.56 and parameters: {'n_estimators': 104, 'max_depth': 217, 'min_samples_split': 5, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:08,254] Trial 61 finished with value: 0.6 and parameters: {'n_estimators': 174, 'max_depth': 131, 'min_samples_split': 4, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:08,350] Trial 62 finished with value: 0.56 and parameters: {'n_estimators': 159, 'max_depth': 157, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:08,417] Trial 63 finished with value: 0.6 and parameters: {'n_estimators': 136, 'max_depth': 122, 'min_samples_split': 5, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:08,512] Trial 64 finished with value: 0.56 and parameters: {'n_estimators': 205, 'max_depth': 97, 'min_samples_split': 11, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:08,609] Trial 65 finished with value: 0.6 and parameters: {'n_estimators': 227, 'max_depth': 228, 'min_samples_split': 3, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:08,686] Trial 66 finished with value: 0.52 and parameters: {'n_estimators': 184, 'max_depth': 208, 'min_samples_split': 10, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:08,755] Trial 67 finished with value: 0.6 and parameters: {'n_estimators': 167, 'max_depth': 137, 'min_samples_split': 6, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:08,834] Trial 68 finished with value: 0.56 and parameters: {'n_estimators': 190, 'max_depth': 243, 'min_samples_split': 8, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:08,915] Trial 69 finished with value: 0.6 and parameters: {'n_estimators': 199, 'max_depth': 231, 'min_samples_split': 9, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:08,986] Trial 70 finished with value: 0.52 and parameters: {'n_estimators': 139, 'max_depth': 171, 'min_samples_split': 5, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:09,072] Trial 71 finished with value: 0.56 and parameters: {'n_estimators': 212, 'max_depth': 213, 'min_samples_split': 11, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:09,151] Trial 72 finished with value: 0.52 and parameters: {'n_estimators': 197, 'max_depth': 221, 'min_samples_split': 12, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:09,224] Trial 73 finished with value: 0.56 and parameters: {'n_estimators': 176, 'max_depth': 182, 'min_samples_split': 10, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:09,300] Trial 74 finished with value: 0.56 and parameters: {'n_estimators': 188, 'max_depth': 250, 'min_samples_split': 12, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:09,387] Trial 75 finished with value: 0.52 and parameters: {'n_estimators': 220, 'max_depth': 195, 'min_samples_split': 4, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:09,478] Trial 76 finished with value: 0.52 and parameters: {'n_estimators': 237, 'max_depth': 232, 'min_samples_split': 10, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:09,557] Trial 77 finished with value: 0.56 and parameters: {'n_estimators': 155, 'max_depth': 215, 'min_samples_split': 15, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:09,621] Trial 78 finished with value: 0.48 and parameters: {'n_estimators': 125, 'max_depth': 204, 'min_samples_split': 11, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:09,703] Trial 79 finished with value: 0.6 and parameters: {'n_estimators': 204, 'max_depth': 237, 'min_samples_split': 9, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:09,787] Trial 80 finished with value: 0.52 and parameters: {'n_estimators': 165, 'max_depth': 189, 'min_samples_split': 3, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:09,865] Trial 81 finished with value: 0.6 and parameters: {'n_estimators': 194, 'max_depth': 157, 'min_samples_split': 11, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:09,950] Trial 82 finished with value: 0.56 and parameters: {'n_estimators': 213, 'max_depth': 165, 'min_samples_split': 12, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:10,022] Trial 83 finished with value: 0.6 and parameters: {'n_estimators': 173, 'max_depth': 226, 'min_samples_split': 10, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:10,097] Trial 84 finished with value: 0.6 and parameters: {'n_estimators': 184, 'max_depth': 147, 'min_samples_split': 11, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:10,183] Trial 85 finished with value: 0.56 and parameters: {'n_estimators': 208, 'max_depth': 153, 'min_samples_split': 9, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:10,225] Trial 86 finished with value: 0.64 and parameters: {'n_estimators': 79, 'max_depth': 244, 'min_samples_split': 13, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:10,262] Trial 87 finished with value: 0.56 and parameters: {'n_estimators': 51, 'max_depth': 241, 'min_samples_split': 14, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:10,299] Trial 88 finished with value: 0.56 and parameters: {'n_estimators': 63, 'max_depth': 244, 'min_samples_split': 14, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:10,329] Trial 89 finished with value: 0.6 and parameters: {'n_estimators': 37, 'max_depth': 221, 'min_samples_split': 2, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:10,373] Trial 90 finished with value: 0.6 and parameters: {'n_estimators': 83, 'max_depth': 235, 'min_samples_split': 13, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:10,450] Trial 91 finished with value: 0.6 and parameters: {'n_estimators': 176, 'max_depth': 183, 'min_samples_split': 7, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:10,543] Trial 92 finished with value: 0.6 and parameters: {'n_estimators': 231, 'max_depth': 200, 'min_samples_split': 12, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:10,621] Trial 93 finished with value: 0.6 and parameters: {'n_estimators': 188, 'max_depth': 247, 'min_samples_split': 6, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:10,699] Trial 94 finished with value: 0.56 and parameters: {'n_estimators': 154, 'max_depth': 211, 'min_samples_split': 5, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:10,720] Trial 95 finished with value: 0.56 and parameters: {'n_estimators': 13, 'max_depth': 173, 'min_samples_split': 8, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:10,820] Trial 96 finished with value: 0.56 and parameters: {'n_estimators': 219, 'max_depth': 161, 'min_samples_split': 13, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:10,893] Trial 97 finished with value: 0.56 and parameters: {'n_estimators': 144, 'max_depth': 228, 'min_samples_split': 10, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:10,992] Trial 98 finished with value: 0.56 and parameters: {'n_estimators': 162, 'max_depth': 219, 'min_samples_split': 11, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:11,159] Trial 99 finished with value: 0.56 and parameters: {'n_estimators': 195, 'max_depth': 237, 'min_samples_split': 4, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 21 with value: 0.64.\n",
      "[I 2024-10-20 20:58:14,407] A new study created in memory with name: no-name-13e73763-2225-486a-966b-ccd6cbae1455\n",
      "[I 2024-10-20 20:58:14,647] Trial 0 finished with value: 0.6 and parameters: {'n_estimators': 263, 'max_depth': 24, 'min_samples_split': 3, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 0 with value: 0.6.\n",
      "[I 2024-10-20 20:58:14,706] Trial 1 finished with value: 0.6 and parameters: {'n_estimators': 110, 'max_depth': 189, 'min_samples_split': 8, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 0 with value: 0.6.\n",
      "[I 2024-10-20 20:58:14,728] Trial 2 finished with value: 0.56 and parameters: {'n_estimators': 45, 'max_depth': 30, 'min_samples_split': 9, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 0 with value: 0.6.\n",
      "[I 2024-10-20 20:58:14,860] Trial 3 finished with value: 0.6 and parameters: {'n_estimators': 299, 'max_depth': 33, 'min_samples_split': 14, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 0 with value: 0.6.\n",
      "[I 2024-10-20 20:58:14,920] Trial 4 finished with value: 0.44 and parameters: {'n_estimators': 171, 'max_depth': 147, 'min_samples_split': 2, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 0 with value: 0.6.\n",
      "[I 2024-10-20 20:58:15,022] Trial 5 finished with value: 0.6 and parameters: {'n_estimators': 273, 'max_depth': 71, 'min_samples_split': 8, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 0 with value: 0.6.\n",
      "[I 2024-10-20 20:58:15,136] Trial 6 finished with value: 0.56 and parameters: {'n_estimators': 229, 'max_depth': 36, 'min_samples_split': 4, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 0 with value: 0.6.\n",
      "[I 2024-10-20 20:58:15,208] Trial 7 finished with value: 0.6 and parameters: {'n_estimators': 192, 'max_depth': 19, 'min_samples_split': 9, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 0 with value: 0.6.\n",
      "[I 2024-10-20 20:58:15,219] Trial 8 finished with value: 0.64 and parameters: {'n_estimators': 27, 'max_depth': 224, 'min_samples_split': 4, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:15,250] Trial 9 finished with value: 0.56 and parameters: {'n_estimators': 62, 'max_depth': 42, 'min_samples_split': 4, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:15,268] Trial 10 finished with value: 0.44 and parameters: {'n_estimators': 18, 'max_depth': 244, 'min_samples_split': 6, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:15,316] Trial 11 finished with value: 0.56 and parameters: {'n_estimators': 111, 'max_depth': 121, 'min_samples_split': 2, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:15,404] Trial 12 finished with value: 0.56 and parameters: {'n_estimators': 228, 'max_depth': 246, 'min_samples_split': 5, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:15,453] Trial 13 finished with value: 0.64 and parameters: {'n_estimators': 115, 'max_depth': 189, 'min_samples_split': 14, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:15,499] Trial 14 finished with value: 0.6 and parameters: {'n_estimators': 103, 'max_depth': 198, 'min_samples_split': 15, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:15,534] Trial 15 finished with value: 0.56 and parameters: {'n_estimators': 75, 'max_depth': 200, 'min_samples_split': 12, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:15,593] Trial 16 finished with value: 0.64 and parameters: {'n_estimators': 144, 'max_depth': 157, 'min_samples_split': 12, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:15,607] Trial 17 finished with value: 0.64 and parameters: {'n_estimators': 11, 'max_depth': 211, 'min_samples_split': 11, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:15,679] Trial 18 finished with value: 0.48 and parameters: {'n_estimators': 146, 'max_depth': 101, 'min_samples_split': 6, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:15,717] Trial 19 finished with value: 0.6 and parameters: {'n_estimators': 80, 'max_depth': 172, 'min_samples_split': 14, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:15,739] Trial 20 finished with value: 0.6 and parameters: {'n_estimators': 35, 'max_depth': 223, 'min_samples_split': 10, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:15,801] Trial 21 finished with value: 0.64 and parameters: {'n_estimators': 141, 'max_depth': 162, 'min_samples_split': 12, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:15,872] Trial 22 finished with value: 0.6 and parameters: {'n_estimators': 179, 'max_depth': 173, 'min_samples_split': 13, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:15,928] Trial 23 finished with value: 0.6 and parameters: {'n_estimators': 133, 'max_depth': 130, 'min_samples_split': 15, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:15,968] Trial 24 finished with value: 0.6 and parameters: {'n_estimators': 86, 'max_depth': 223, 'min_samples_split': 11, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:16,022] Trial 25 finished with value: 0.6 and parameters: {'n_estimators': 119, 'max_depth': 153, 'min_samples_split': 13, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:16,129] Trial 26 finished with value: 0.56 and parameters: {'n_estimators': 209, 'max_depth': 113, 'min_samples_split': 7, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:16,318] Trial 27 finished with value: 0.6 and parameters: {'n_estimators': 162, 'max_depth': 181, 'min_samples_split': 10, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:16,406] Trial 28 finished with value: 0.64 and parameters: {'n_estimators': 50, 'max_depth': 228, 'min_samples_split': 13, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:16,545] Trial 29 finished with value: 0.56 and parameters: {'n_estimators': 92, 'max_depth': 139, 'min_samples_split': 14, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:16,622] Trial 30 finished with value: 0.48 and parameters: {'n_estimators': 123, 'max_depth': 89, 'min_samples_split': 11, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:16,656] Trial 31 finished with value: 0.6 and parameters: {'n_estimators': 27, 'max_depth': 213, 'min_samples_split': 11, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 8 with value: 0.64.\n",
      "[I 2024-10-20 20:58:16,680] Trial 32 finished with value: 0.72 and parameters: {'n_estimators': 12, 'max_depth': 203, 'min_samples_split': 12, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:16,716] Trial 33 finished with value: 0.6 and parameters: {'n_estimators': 65, 'max_depth': 193, 'min_samples_split': 12, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:16,748] Trial 34 finished with value: 0.6 and parameters: {'n_estimators': 48, 'max_depth': 165, 'min_samples_split': 15, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:16,772] Trial 35 finished with value: 0.64 and parameters: {'n_estimators': 34, 'max_depth': 184, 'min_samples_split': 10, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:16,788] Trial 36 finished with value: 0.52 and parameters: {'n_estimators': 11, 'max_depth': 237, 'min_samples_split': 14, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:16,843] Trial 37 finished with value: 0.52 and parameters: {'n_estimators': 97, 'max_depth': 202, 'min_samples_split': 8, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:16,910] Trial 38 finished with value: 0.64 and parameters: {'n_estimators': 158, 'max_depth': 154, 'min_samples_split': 9, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:16,997] Trial 39 finished with value: 0.64 and parameters: {'n_estimators': 198, 'max_depth': 212, 'min_samples_split': 3, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,037] Trial 40 finished with value: 0.48 and parameters: {'n_estimators': 62, 'max_depth': 182, 'min_samples_split': 7, 'min_samples_leaf': 3, 'bootstrap': True}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,055] Trial 41 finished with value: 0.56 and parameters: {'n_estimators': 17, 'max_depth': 207, 'min_samples_split': 12, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,082] Trial 42 finished with value: 0.6 and parameters: {'n_estimators': 45, 'max_depth': 231, 'min_samples_split': 13, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,105] Trial 43 finished with value: 0.68 and parameters: {'n_estimators': 28, 'max_depth': 217, 'min_samples_split': 12, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,128] Trial 44 finished with value: 0.64 and parameters: {'n_estimators': 29, 'max_depth': 193, 'min_samples_split': 14, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,161] Trial 45 finished with value: 0.64 and parameters: {'n_estimators': 58, 'max_depth': 220, 'min_samples_split': 12, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,198] Trial 46 finished with value: 0.6 and parameters: {'n_estimators': 72, 'max_depth': 242, 'min_samples_split': 10, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,219] Trial 47 finished with value: 0.6 and parameters: {'n_estimators': 27, 'max_depth': 250, 'min_samples_split': 4, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,251] Trial 48 finished with value: 0.52 and parameters: {'n_estimators': 44, 'max_depth': 142, 'min_samples_split': 13, 'min_samples_leaf': 2, 'bootstrap': True}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,327] Trial 49 finished with value: 0.6 and parameters: {'n_estimators': 176, 'max_depth': 176, 'min_samples_split': 2, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,424] Trial 50 finished with value: 0.6 and parameters: {'n_estimators': 244, 'max_depth': 51, 'min_samples_split': 9, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,441] Trial 51 finished with value: 0.64 and parameters: {'n_estimators': 13, 'max_depth': 215, 'min_samples_split': 11, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,557] Trial 52 finished with value: 0.6 and parameters: {'n_estimators': 298, 'max_depth': 206, 'min_samples_split': 12, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,577] Trial 53 finished with value: 0.52 and parameters: {'n_estimators': 22, 'max_depth': 232, 'min_samples_split': 11, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,604] Trial 54 finished with value: 0.64 and parameters: {'n_estimators': 41, 'max_depth': 191, 'min_samples_split': 14, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,662] Trial 55 finished with value: 0.56 and parameters: {'n_estimators': 135, 'max_depth': 197, 'min_samples_split': 13, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,716] Trial 56 finished with value: 0.6 and parameters: {'n_estimators': 114, 'max_depth': 219, 'min_samples_split': 15, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,733] Trial 57 finished with value: 0.48 and parameters: {'n_estimators': 11, 'max_depth': 165, 'min_samples_split': 10, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,769] Trial 58 finished with value: 0.4 and parameters: {'n_estimators': 54, 'max_depth': 206, 'min_samples_split': 5, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,833] Trial 59 finished with value: 0.6 and parameters: {'n_estimators': 149, 'max_depth': 187, 'min_samples_split': 12, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,860] Trial 60 finished with value: 0.6 and parameters: {'n_estimators': 39, 'max_depth': 238, 'min_samples_split': 11, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,916] Trial 61 finished with value: 0.64 and parameters: {'n_estimators': 129, 'max_depth': 160, 'min_samples_split': 12, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:17,985] Trial 62 finished with value: 0.6 and parameters: {'n_estimators': 165, 'max_depth': 168, 'min_samples_split': 12, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,045] Trial 63 finished with value: 0.56 and parameters: {'n_estimators': 141, 'max_depth': 134, 'min_samples_split': 13, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,065] Trial 64 finished with value: 0.68 and parameters: {'n_estimators': 21, 'max_depth': 148, 'min_samples_split': 14, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,086] Trial 65 finished with value: 0.64 and parameters: {'n_estimators': 22, 'max_depth': 155, 'min_samples_split': 14, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,112] Trial 66 finished with value: 0.52 and parameters: {'n_estimators': 34, 'max_depth': 118, 'min_samples_split': 15, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,163] Trial 67 finished with value: 0.6 and parameters: {'n_estimators': 104, 'max_depth': 225, 'min_samples_split': 7, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,203] Trial 68 finished with value: 0.6 and parameters: {'n_estimators': 77, 'max_depth': 199, 'min_samples_split': 13, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,225] Trial 69 finished with value: 0.48 and parameters: {'n_estimators': 20, 'max_depth': 127, 'min_samples_split': 14, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,250] Trial 70 finished with value: 0.52 and parameters: {'n_estimators': 34, 'max_depth': 148, 'min_samples_split': 9, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,267] Trial 71 finished with value: 0.56 and parameters: {'n_estimators': 10, 'max_depth': 178, 'min_samples_split': 11, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,333] Trial 72 finished with value: 0.6 and parameters: {'n_estimators': 155, 'max_depth': 147, 'min_samples_split': 13, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,411] Trial 73 finished with value: 0.56 and parameters: {'n_estimators': 185, 'max_depth': 172, 'min_samples_split': 12, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,447] Trial 74 finished with value: 0.6 and parameters: {'n_estimators': 67, 'max_depth': 211, 'min_samples_split': 3, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,489] Trial 75 finished with value: 0.6 and parameters: {'n_estimators': 84, 'max_depth': 159, 'min_samples_split': 15, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,512] Trial 76 finished with value: 0.56 and parameters: {'n_estimators': 26, 'max_depth': 11, 'min_samples_split': 11, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,546] Trial 77 finished with value: 0.56 and parameters: {'n_estimators': 50, 'max_depth': 188, 'min_samples_split': 14, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,608] Trial 78 finished with value: 0.6 and parameters: {'n_estimators': 141, 'max_depth': 108, 'min_samples_split': 10, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,694] Trial 79 finished with value: 0.6 and parameters: {'n_estimators': 209, 'max_depth': 219, 'min_samples_split': 13, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,760] Trial 80 finished with value: 0.48 and parameters: {'n_estimators': 123, 'max_depth': 138, 'min_samples_split': 8, 'min_samples_leaf': 6, 'bootstrap': True}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,779] Trial 81 finished with value: 0.6 and parameters: {'n_estimators': 17, 'max_depth': 228, 'min_samples_split': 13, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,804] Trial 82 finished with value: 0.6 and parameters: {'n_estimators': 29, 'max_depth': 235, 'min_samples_split': 12, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,839] Trial 83 finished with value: 0.6 and parameters: {'n_estimators': 48, 'max_depth': 214, 'min_samples_split': 13, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,871] Trial 84 finished with value: 0.56 and parameters: {'n_estimators': 56, 'max_depth': 245, 'min_samples_split': 12, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,899] Trial 85 finished with value: 0.64 and parameters: {'n_estimators': 37, 'max_depth': 203, 'min_samples_split': 14, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,919] Trial 86 finished with value: 0.52 and parameters: {'n_estimators': 19, 'max_depth': 227, 'min_samples_split': 13, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,944] Trial 87 finished with value: 0.6 and parameters: {'n_estimators': 28, 'max_depth': 197, 'min_samples_split': 5, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:18,993] Trial 88 finished with value: 0.6 and parameters: {'n_estimators': 94, 'max_depth': 182, 'min_samples_split': 11, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:19,066] Trial 89 finished with value: 0.52 and parameters: {'n_estimators': 169, 'max_depth': 210, 'min_samples_split': 12, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:19,086] Trial 90 finished with value: 0.52 and parameters: {'n_estimators': 10, 'max_depth': 222, 'min_samples_split': 6, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:19,114] Trial 91 finished with value: 0.64 and parameters: {'n_estimators': 35, 'max_depth': 185, 'min_samples_split': 10, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:19,159] Trial 92 finished with value: 0.56 and parameters: {'n_estimators': 42, 'max_depth': 168, 'min_samples_split': 11, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:19,297] Trial 93 finished with value: 0.56 and parameters: {'n_estimators': 151, 'max_depth': 195, 'min_samples_split': 12, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:19,327] Trial 94 finished with value: 0.52 and parameters: {'n_estimators': 16, 'max_depth': 175, 'min_samples_split': 10, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:19,368] Trial 95 finished with value: 0.64 and parameters: {'n_estimators': 26, 'max_depth': 160, 'min_samples_split': 11, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:19,408] Trial 96 finished with value: 0.64 and parameters: {'n_estimators': 61, 'max_depth': 216, 'min_samples_split': 14, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:19,488] Trial 97 finished with value: 0.48 and parameters: {'n_estimators': 130, 'max_depth': 149, 'min_samples_split': 13, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:19,513] Trial 98 finished with value: 0.56 and parameters: {'n_estimators': 32, 'max_depth': 204, 'min_samples_split': 12, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n",
      "[I 2024-10-20 20:58:19,538] Trial 99 finished with value: 0.56 and parameters: {'n_estimators': 23, 'max_depth': 230, 'min_samples_split': 12, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 32 with value: 0.72.\n"
     ]
    }
   ],
   "source": [
    "y_preds_final = []\n",
    "\n",
    "for j in range(5):\n",
    "    #f_training_X_img = df_training_X[[str(j) for j in range(2048*j, 2048 + 2048*j)]]\n",
    "    #df_training_y_img = downsampled_df['price_bucket'].apply(convert_price)\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(df_training_X_img, df_training_y_img, test_size=.25, random_state=22)\n",
    "    X_train_PCA_meta, X_test_PCA_meta = run_PCA(X_train[[str(i) for i in range(2048*j, 2048 + 2048*j)]], X_test[[str(i) for i in range(2048*j, 2048 + 2048*j)]], use_all=False, num_components=12)\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 10, 300),          \n",
    "            'max_depth': trial.suggest_int('max_depth', 10, 250),            \n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 15, step=1),  \n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 8, step=1),      \n",
    "            'bootstrap': trial.suggest_categorical('bootstrap', [True, False])    \n",
    "        }\n",
    "\n",
    "        model = RandomForestClassifier(**params)\n",
    "        model.fit(X_train_PCA_meta, y_train)\n",
    "        y_preds_RF = model.predict(X_test_PCA_meta)\n",
    "\n",
    "        error = accuracy_score(y_test, y_preds_RF)\n",
    "\n",
    "        return error\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    model = RandomForestClassifier(**study.best_params)\n",
    "\n",
    "    model.fit(X_train_PCA_meta, y_train)\n",
    "\n",
    "    with open(f'random_forest_model_{j}.pkl', 'wb') as file:\n",
    "       pickle.dump(model, file)\n",
    "    \n",
    "    y_preds_RF = model.predict(X_test_PCA_meta)\n",
    "    y_preds_final.append(y_preds_RF)\n",
    "\n",
    "meta_matrix = pd.DataFrame(y_preds_final)\n",
    "y_preds_META_MODEL = [meta_matrix[i].mode().squeeze() for i in range(25)]\n",
    "error_RF_meta = accuracy_score(y_test, y_preds_META_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_RF_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seed(22)\n",
    "\n",
    "def create_MLP(num_layers=2, dropout_rate=.5, units=16):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=32, input_dim=X_train_PCA.shape[1]))\n",
    "    for i in range(num_layers):\n",
    "        model.add(Dense(units, activation='tanh'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    return model\n",
    "\n",
    "def objective_nn(trial):\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3, step=2)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', .2, .7)\n",
    "    units = trial.suggest_int('units', 1, 16, step=4)\n",
    "    model = create_MLP(num_layers, dropout_rate, units)\n",
    "    model.fit(X_train_PCA, y_train, epochs=200)\n",
    "    y_preds_MLP = model.predict(X_test_PCA)\n",
    "    y_preds_MLP = (y_preds_MLP > 0.5).astype(int)\n",
    "    \n",
    "    loss = accuracy_score(y_test, y_preds_MLP)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective_nn, n_trials=20)\n",
    "\n",
    "set_seed(22)\n",
    "\n",
    "best_parms = study.best_params\n",
    "model = create_MLP(**best_parms)\n",
    "\n",
    "model.fit(X_train_PCA, y_train, epochs=200)\n",
    "y_preds_MLP = model.predict(X_test_PCA)\n",
    "y_preds_MLP = (y_preds_MLP > 0.5).astype(int)\n",
    "error_NN_hyperparam = accuracy_score(y_test, y_preds_MLP)\n",
    "\n",
    "print(error_NN_hyperparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7392\n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 0s 926us/step - loss: 0.7416\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7548\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7344\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7953\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 0s 988us/step - loss: 0.7533\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 0s 844us/step - loss: 0.8362\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 0s 851us/step - loss: 0.7679\n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8335\n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.8551\n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7998\n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.7054\n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7397\n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7521\n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.7354\n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.8223\n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.8011\n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7316\n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7683\n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.8034\n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8062\n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.8079\n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6854\n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7367\n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7170\n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7028\n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6979\n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7501\n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6614\n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 0s 983us/step - loss: 0.7695\n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 0s 866us/step - loss: 0.6851\n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 0s 800us/step - loss: 0.7127\n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 0s 880us/step - loss: 0.7454\n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 0s 875us/step - loss: 0.7268\n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 0s 852us/step - loss: 0.7283\n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 0s 819us/step - loss: 0.7217\n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 0s 790us/step - loss: 0.6842\n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 0s 818us/step - loss: 0.6603\n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6747\n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7051\n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7844\n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8098\n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7042\n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7021\n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7027\n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7164\n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 0s 863us/step - loss: 0.7376\n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 0s 894us/step - loss: 0.7222\n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7500\n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 0s 974us/step - loss: 0.7303\n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6674\n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7228\n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 0s 855us/step - loss: 0.7188\n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 0s 901us/step - loss: 0.6279\n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7029\n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6739\n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7225\n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 0s 879us/step - loss: 0.6694\n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7518\n",
      "Epoch 60/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7377\n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6313\n",
      "Epoch 62/200\n",
      "3/3 [==============================] - 0s 841us/step - loss: 0.7192\n",
      "Epoch 63/200\n",
      "3/3 [==============================] - 0s 876us/step - loss: 0.6784\n",
      "Epoch 64/200\n",
      "3/3 [==============================] - 0s 780us/step - loss: 0.7546\n",
      "Epoch 65/200\n",
      "3/3 [==============================] - 0s 793us/step - loss: 0.6374\n",
      "Epoch 66/200\n",
      "3/3 [==============================] - 0s 903us/step - loss: 0.6597\n",
      "Epoch 67/200\n",
      "3/3 [==============================] - 0s 822us/step - loss: 0.7210\n",
      "Epoch 68/200\n",
      "3/3 [==============================] - 0s 971us/step - loss: 0.6950\n",
      "Epoch 69/200\n",
      "3/3 [==============================] - 0s 700us/step - loss: 0.6321\n",
      "Epoch 70/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7308\n",
      "Epoch 71/200\n",
      "3/3 [==============================] - 0s 781us/step - loss: 0.6879\n",
      "Epoch 72/200\n",
      "3/3 [==============================] - 0s 730us/step - loss: 0.6587\n",
      "Epoch 73/200\n",
      "3/3 [==============================] - 0s 876us/step - loss: 0.7294\n",
      "Epoch 74/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6615\n",
      "Epoch 75/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6324\n",
      "Epoch 76/200\n",
      "3/3 [==============================] - 0s 921us/step - loss: 0.7185\n",
      "Epoch 77/200\n",
      "3/3 [==============================] - 0s 806us/step - loss: 0.6473\n",
      "Epoch 78/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7047\n",
      "Epoch 79/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6887\n",
      "Epoch 80/200\n",
      "3/3 [==============================] - 0s 742us/step - loss: 0.6859\n",
      "Epoch 81/200\n",
      "3/3 [==============================] - 0s 844us/step - loss: 0.6537\n",
      "Epoch 82/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6556\n",
      "Epoch 83/200\n",
      "3/3 [==============================] - 0s 724us/step - loss: 0.7090\n",
      "Epoch 84/200\n",
      "3/3 [==============================] - 0s 798us/step - loss: 0.7303\n",
      "Epoch 85/200\n",
      "3/3 [==============================] - 0s 674us/step - loss: 0.7481\n",
      "Epoch 86/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7121\n",
      "Epoch 87/200\n",
      "3/3 [==============================] - 0s 956us/step - loss: 0.7100\n",
      "Epoch 88/200\n",
      "3/3 [==============================] - 0s 898us/step - loss: 0.6830\n",
      "Epoch 89/200\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.7470\n",
      "Epoch 90/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.6585\n",
      "Epoch 91/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7075\n",
      "Epoch 92/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7414\n",
      "Epoch 93/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6905\n",
      "Epoch 94/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6409\n",
      "Epoch 95/200\n",
      "3/3 [==============================] - 0s 917us/step - loss: 0.6529\n",
      "Epoch 96/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7390\n",
      "Epoch 97/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6290\n",
      "Epoch 98/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6129\n",
      "Epoch 99/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6938\n",
      "Epoch 100/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7541\n",
      "Epoch 101/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7019\n",
      "Epoch 102/200\n",
      "3/3 [==============================] - 0s 781us/step - loss: 0.6414\n",
      "Epoch 103/200\n",
      "3/3 [==============================] - 0s 962us/step - loss: 0.6329\n",
      "Epoch 104/200\n",
      "3/3 [==============================] - 0s 916us/step - loss: 0.7174\n",
      "Epoch 105/200\n",
      "3/3 [==============================] - 0s 790us/step - loss: 0.6232\n",
      "Epoch 106/200\n",
      "3/3 [==============================] - 0s 847us/step - loss: 0.6512\n",
      "Epoch 107/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5958\n",
      "Epoch 108/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6524\n",
      "Epoch 109/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6544\n",
      "Epoch 110/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7451\n",
      "Epoch 111/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6597\n",
      "Epoch 112/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6465\n",
      "Epoch 113/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7138\n",
      "Epoch 114/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6657\n",
      "Epoch 115/200\n",
      "3/3 [==============================] - 0s 828us/step - loss: 0.6482\n",
      "Epoch 116/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6502\n",
      "Epoch 117/200\n",
      "3/3 [==============================] - 0s 824us/step - loss: 0.7165\n",
      "Epoch 118/200\n",
      "3/3 [==============================] - 0s 753us/step - loss: 0.6498\n",
      "Epoch 119/200\n",
      "3/3 [==============================] - 0s 778us/step - loss: 0.6681\n",
      "Epoch 120/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7275\n",
      "Epoch 121/200\n",
      "3/3 [==============================] - 0s 890us/step - loss: 0.6864\n",
      "Epoch 122/200\n",
      "3/3 [==============================] - 0s 744us/step - loss: 0.6655\n",
      "Epoch 123/200\n",
      "3/3 [==============================] - 0s 804us/step - loss: 0.6257\n",
      "Epoch 124/200\n",
      "3/3 [==============================] - 0s 757us/step - loss: 0.7008\n",
      "Epoch 125/200\n",
      "3/3 [==============================] - 0s 870us/step - loss: 0.6878\n",
      "Epoch 126/200\n",
      "3/3 [==============================] - 0s 769us/step - loss: 0.6047\n",
      "Epoch 127/200\n",
      "3/3 [==============================] - 0s 783us/step - loss: 0.6601\n",
      "Epoch 128/200\n",
      "3/3 [==============================] - 0s 933us/step - loss: 0.6404\n",
      "Epoch 129/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6099\n",
      "Epoch 130/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6603\n",
      "Epoch 131/200\n",
      "3/3 [==============================] - 0s 817us/step - loss: 0.6335\n",
      "Epoch 132/200\n",
      "3/3 [==============================] - 0s 769us/step - loss: 0.7101\n",
      "Epoch 133/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7135\n",
      "Epoch 134/200\n",
      "3/3 [==============================] - 0s 889us/step - loss: 0.6436\n",
      "Epoch 135/200\n",
      "3/3 [==============================] - 0s 818us/step - loss: 0.6395\n",
      "Epoch 136/200\n",
      "3/3 [==============================] - 0s 751us/step - loss: 0.6360\n",
      "Epoch 137/200\n",
      "3/3 [==============================] - 0s 916us/step - loss: 0.7069\n",
      "Epoch 138/200\n",
      "3/3 [==============================] - 0s 831us/step - loss: 0.6906\n",
      "Epoch 139/200\n",
      "3/3 [==============================] - 0s 773us/step - loss: 0.6628\n",
      "Epoch 140/200\n",
      "3/3 [==============================] - 0s 771us/step - loss: 0.6334\n",
      "Epoch 141/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6580\n",
      "Epoch 142/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6317\n",
      "Epoch 143/200\n",
      "3/3 [==============================] - 0s 830us/step - loss: 0.6715\n",
      "Epoch 144/200\n",
      "3/3 [==============================] - 0s 822us/step - loss: 0.6582\n",
      "Epoch 145/200\n",
      "3/3 [==============================] - 0s 899us/step - loss: 0.6370\n",
      "Epoch 146/200\n",
      "3/3 [==============================] - 0s 884us/step - loss: 0.6577\n",
      "Epoch 147/200\n",
      "3/3 [==============================] - 0s 800us/step - loss: 0.6453\n",
      "Epoch 148/200\n",
      "3/3 [==============================] - 0s 970us/step - loss: 0.6267\n",
      "Epoch 149/200\n",
      "3/3 [==============================] - 0s 879us/step - loss: 0.6611\n",
      "Epoch 150/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6126\n",
      "Epoch 151/200\n",
      "3/3 [==============================] - 0s 752us/step - loss: 0.6017\n",
      "Epoch 152/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6443\n",
      "Epoch 153/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6598\n",
      "Epoch 154/200\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6398\n",
      "Epoch 155/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6846\n",
      "Epoch 156/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.6493\n",
      "Epoch 157/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6404\n",
      "Epoch 158/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6360\n",
      "Epoch 159/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6703\n",
      "Epoch 160/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6142\n",
      "Epoch 161/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6734\n",
      "Epoch 162/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6785\n",
      "Epoch 163/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6255\n",
      "Epoch 164/200\n",
      "3/3 [==============================] - 0s 898us/step - loss: 0.6384\n",
      "Epoch 165/200\n",
      "3/3 [==============================] - 0s 954us/step - loss: 0.6705\n",
      "Epoch 166/200\n",
      "3/3 [==============================] - 0s 844us/step - loss: 0.6185\n",
      "Epoch 167/200\n",
      "3/3 [==============================] - 0s 941us/step - loss: 0.6299\n",
      "Epoch 168/200\n",
      "3/3 [==============================] - 0s 783us/step - loss: 0.6551\n",
      "Epoch 169/200\n",
      "3/3 [==============================] - 0s 776us/step - loss: 0.6104\n",
      "Epoch 170/200\n",
      "3/3 [==============================] - 0s 801us/step - loss: 0.7045\n",
      "Epoch 171/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6218\n",
      "Epoch 172/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6453\n",
      "Epoch 173/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6446\n",
      "Epoch 174/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6310\n",
      "Epoch 175/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5870\n",
      "Epoch 176/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6124\n",
      "Epoch 177/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6719\n",
      "Epoch 178/200\n",
      "3/3 [==============================] - 0s 786us/step - loss: 0.6510\n",
      "Epoch 179/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6611\n",
      "Epoch 180/200\n",
      "3/3 [==============================] - 0s 775us/step - loss: 0.6393\n",
      "Epoch 181/200\n",
      "3/3 [==============================] - 0s 787us/step - loss: 0.6396\n",
      "Epoch 182/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6358\n",
      "Epoch 183/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6359\n",
      "Epoch 184/200\n",
      "3/3 [==============================] - 0s 927us/step - loss: 0.6438\n",
      "Epoch 185/200\n",
      "3/3 [==============================] - 0s 837us/step - loss: 0.5932\n",
      "Epoch 186/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6980\n",
      "Epoch 187/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5992\n",
      "Epoch 188/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6011\n",
      "Epoch 189/200\n",
      "3/3 [==============================] - 0s 869us/step - loss: 0.6241\n",
      "Epoch 190/200\n",
      "3/3 [==============================] - 0s 919us/step - loss: 0.6399\n",
      "Epoch 191/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6299\n",
      "Epoch 192/200\n",
      "3/3 [==============================] - 0s 852us/step - loss: 0.6674\n",
      "Epoch 193/200\n",
      "3/3 [==============================] - 0s 741us/step - loss: 0.6701\n",
      "Epoch 194/200\n",
      "3/3 [==============================] - 0s 786us/step - loss: 0.6360\n",
      "Epoch 195/200\n",
      "3/3 [==============================] - 0s 916us/step - loss: 0.6591\n",
      "Epoch 196/200\n",
      "3/3 [==============================] - 0s 779us/step - loss: 0.6159\n",
      "Epoch 197/200\n",
      "3/3 [==============================] - 0s 930us/step - loss: 0.6172\n",
      "Epoch 198/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6136\n",
      "Epoch 199/200\n",
      "3/3 [==============================] - 0s 878us/step - loss: 0.6288\n",
      "Epoch 200/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6225\n",
      "1/1 [==============================] - 0s 36ms/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(8, input_dim = X_train_PCA.shape[1], activation='tanh'))\n",
    "model.add(Dropout(0.5)) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "model.fit(X_train_PCA, y_train, epochs=200)\n",
    "\n",
    "y_preds_MLP = model.predict(X_test_PCA)\n",
    "y_preds_MLP = (y_preds_MLP > 0.5).astype(int)\n",
    "error_NN = accuracy_score(y_test, y_preds_MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8073\n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8544\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 0s 985us/step - loss: 0.6771\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7561\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 0s 920us/step - loss: 0.8087\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8370\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7823\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7346\n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 0s 987us/step - loss: 0.6908\n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 0s 826us/step - loss: 0.7263\n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 0s 989us/step - loss: 0.8434\n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7841\n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 0s 942us/step - loss: 0.8065\n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 0s 882us/step - loss: 0.7047\n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8514\n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8239\n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7648\n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7473\n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8692\n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7944\n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7001\n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7561\n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 0s 994us/step - loss: 0.7643\n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7888\n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 0s 986us/step - loss: 0.7093\n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8482\n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.8022\n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.7687\n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.7380\n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7309\n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.7908\n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7033\n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.7442\n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.7213\n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.8101\n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.7956\n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7202\n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7104\n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8656\n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7392\n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7414\n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6878\n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7397\n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6873\n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8262\n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7385\n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 0s 993us/step - loss: 0.7829\n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7720\n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8259\n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7261\n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7724\n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7170\n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7584\n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.7094\n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6874\n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6904\n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7402\n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6346\n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6897\n",
      "Epoch 60/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7062\n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6627\n",
      "Epoch 62/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8911\n",
      "Epoch 63/200\n",
      "3/3 [==============================] - 0s 915us/step - loss: 0.7495\n",
      "Epoch 64/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7814\n",
      "Epoch 65/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6859\n",
      "Epoch 66/200\n",
      "3/3 [==============================] - 0s 951us/step - loss: 0.6516\n",
      "Epoch 67/200\n",
      "3/3 [==============================] - 0s 883us/step - loss: 0.6661\n",
      "Epoch 68/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7221\n",
      "Epoch 69/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6912\n",
      "Epoch 70/200\n",
      "3/3 [==============================] - 0s 889us/step - loss: 0.7644\n",
      "Epoch 71/200\n",
      "3/3 [==============================] - 0s 875us/step - loss: 0.7443\n",
      "Epoch 72/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7189\n",
      "Epoch 73/200\n",
      "3/3 [==============================] - 0s 920us/step - loss: 0.7552\n",
      "Epoch 74/200\n",
      "3/3 [==============================] - 0s 943us/step - loss: 0.7681\n",
      "Epoch 75/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7030\n",
      "Epoch 76/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7024\n",
      "Epoch 77/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7697\n",
      "Epoch 78/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7114\n",
      "Epoch 79/200\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.7334\n",
      "Epoch 80/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.7400\n",
      "Epoch 81/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6770\n",
      "Epoch 82/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7132\n",
      "Epoch 83/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7051\n",
      "Epoch 84/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7048\n",
      "Epoch 85/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7852\n",
      "Epoch 86/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6497\n",
      "Epoch 87/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7357\n",
      "Epoch 88/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6661\n",
      "Epoch 89/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6760\n",
      "Epoch 90/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6649\n",
      "Epoch 91/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6683\n",
      "Epoch 92/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7551\n",
      "Epoch 93/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7161\n",
      "Epoch 94/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7185\n",
      "Epoch 95/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7179\n",
      "Epoch 96/200\n",
      "3/3 [==============================] - 0s 940us/step - loss: 0.6906\n",
      "Epoch 97/200\n",
      "3/3 [==============================] - 0s 952us/step - loss: 0.7584\n",
      "Epoch 98/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6730\n",
      "Epoch 99/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6961\n",
      "Epoch 100/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6629\n",
      "Epoch 101/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6536\n",
      "Epoch 102/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7106\n",
      "Epoch 103/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7215\n",
      "Epoch 104/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7484\n",
      "Epoch 105/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7112\n",
      "Epoch 106/200\n",
      "3/3 [==============================] - 0s 903us/step - loss: 0.6916\n",
      "Epoch 107/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6903\n",
      "Epoch 108/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7230\n",
      "Epoch 109/200\n",
      "3/3 [==============================] - 0s 884us/step - loss: 0.6811\n",
      "Epoch 110/200\n",
      "3/3 [==============================] - 0s 862us/step - loss: 0.6959\n",
      "Epoch 111/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7274\n",
      "Epoch 112/200\n",
      "3/3 [==============================] - 0s 867us/step - loss: 0.6398\n",
      "Epoch 113/200\n",
      "3/3 [==============================] - 0s 842us/step - loss: 0.7186\n",
      "Epoch 114/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6475\n",
      "Epoch 115/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7554\n",
      "Epoch 116/200\n",
      "3/3 [==============================] - 0s 829us/step - loss: 0.7200\n",
      "Epoch 117/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7006\n",
      "Epoch 118/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6781\n",
      "Epoch 119/200\n",
      "3/3 [==============================] - 0s 887us/step - loss: 0.6221\n",
      "Epoch 120/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7035\n",
      "Epoch 121/200\n",
      "3/3 [==============================] - 0s 844us/step - loss: 0.6650\n",
      "Epoch 122/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6818\n",
      "Epoch 123/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7133\n",
      "Epoch 124/200\n",
      "3/3 [==============================] - 0s 875us/step - loss: 0.7218\n",
      "Epoch 125/200\n",
      "3/3 [==============================] - 0s 855us/step - loss: 0.6684\n",
      "Epoch 126/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6737\n",
      "Epoch 127/200\n",
      "3/3 [==============================] - 0s 878us/step - loss: 0.7057\n",
      "Epoch 128/200\n",
      "3/3 [==============================] - 0s 824us/step - loss: 0.7357\n",
      "Epoch 129/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6502\n",
      "Epoch 130/200\n",
      "3/3 [==============================] - 0s 845us/step - loss: 0.7112\n",
      "Epoch 131/200\n",
      "3/3 [==============================] - 0s 835us/step - loss: 0.6685\n",
      "Epoch 132/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6922\n",
      "Epoch 133/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7384\n",
      "Epoch 134/200\n",
      "3/3 [==============================] - 0s 813us/step - loss: 0.6794\n",
      "Epoch 135/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6737\n",
      "Epoch 136/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6543\n",
      "Epoch 137/200\n",
      "3/3 [==============================] - 0s 974us/step - loss: 0.6407\n",
      "Epoch 138/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7321\n",
      "Epoch 139/200\n",
      "3/3 [==============================] - 0s 897us/step - loss: 0.6884\n",
      "Epoch 140/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7395\n",
      "Epoch 141/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6636\n",
      "Epoch 142/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7076\n",
      "Epoch 143/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6142\n",
      "Epoch 144/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6973\n",
      "Epoch 145/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7122\n",
      "Epoch 146/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6975\n",
      "Epoch 147/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7584\n",
      "Epoch 148/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7338\n",
      "Epoch 149/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.7645\n",
      "Epoch 150/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7078\n",
      "Epoch 151/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6426\n",
      "Epoch 152/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6892\n",
      "Epoch 153/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6370\n",
      "Epoch 154/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7232\n",
      "Epoch 155/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6197\n",
      "Epoch 156/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6671\n",
      "Epoch 157/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6419\n",
      "Epoch 158/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6606\n",
      "Epoch 159/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7213\n",
      "Epoch 160/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6795\n",
      "Epoch 161/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6586\n",
      "Epoch 162/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6611\n",
      "Epoch 163/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7097\n",
      "Epoch 164/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6229\n",
      "Epoch 165/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7040\n",
      "Epoch 166/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6733\n",
      "Epoch 167/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6931\n",
      "Epoch 168/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6560\n",
      "Epoch 169/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6667\n",
      "Epoch 170/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7155\n",
      "Epoch 171/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6783\n",
      "Epoch 172/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6500\n",
      "Epoch 173/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6694\n",
      "Epoch 174/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6277\n",
      "Epoch 175/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7276\n",
      "Epoch 176/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6966\n",
      "Epoch 177/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6815\n",
      "Epoch 178/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7056\n",
      "Epoch 179/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6358\n",
      "Epoch 180/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6339\n",
      "Epoch 181/200\n",
      "3/3 [==============================] - 0s 915us/step - loss: 0.6782\n",
      "Epoch 182/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6573\n",
      "Epoch 183/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6623\n",
      "Epoch 184/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6592\n",
      "Epoch 185/200\n",
      "3/3 [==============================] - 0s 934us/step - loss: 0.6553\n",
      "Epoch 186/200\n",
      "3/3 [==============================] - 0s 971us/step - loss: 0.6394\n",
      "Epoch 187/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7139\n",
      "Epoch 188/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6963\n",
      "Epoch 189/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6737\n",
      "Epoch 190/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6625\n",
      "Epoch 191/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6435\n",
      "Epoch 192/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6662\n",
      "Epoch 193/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6534\n",
      "Epoch 194/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6861\n",
      "Epoch 195/200\n",
      "3/3 [==============================] - 0s 771us/step - loss: 0.7007\n",
      "Epoch 196/200\n",
      "3/3 [==============================] - 0s 901us/step - loss: 0.6135\n",
      "Epoch 197/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6554\n",
      "Epoch 198/200\n",
      "3/3 [==============================] - 0s 961us/step - loss: 0.6706\n",
      "Epoch 199/200\n",
      "3/3 [==============================] - 0s 874us/step - loss: 0.6622\n",
      "Epoch 200/200\n",
      "3/3 [==============================] - 0s 973us/step - loss: 0.6217\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Epoch 1/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.0000\n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 0s 940us/step - loss: 0.8808\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 0s 986us/step - loss: 0.7898\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8260\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7787\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 0s 971us/step - loss: 0.8038\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8496\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7059\n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8966\n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.9009\n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7635\n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8484\n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.8373\n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8034\n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8365\n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7562\n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7456\n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 0s 890us/step - loss: 0.7440\n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 0s 905us/step - loss: 0.8388\n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8495\n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 0s 947us/step - loss: 0.8268\n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7412\n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8900\n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8385\n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 0s 901us/step - loss: 0.7883\n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8592\n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8258\n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8601\n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 0s 874us/step - loss: 0.7470\n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.7804\n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.7488\n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.8075\n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.7471\n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8670\n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.8411\n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7502\n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.7194\n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.7454\n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.7248\n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.8199\n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.7439\n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7150\n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.8398\n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 0s 970us/step - loss: 0.7558\n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7789\n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7599\n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7744\n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7198\n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 0s 980us/step - loss: 0.6728\n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8155\n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 0s 978us/step - loss: 0.7375\n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7466\n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7884\n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 0s 958us/step - loss: 0.7110\n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 0s 975us/step - loss: 0.6445\n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 0s 991us/step - loss: 0.6822\n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 0s 909us/step - loss: 0.7713\n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 0s 940us/step - loss: 0.7293\n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7907\n",
      "Epoch 60/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6932\n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7239\n",
      "Epoch 62/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6603\n",
      "Epoch 63/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7740\n",
      "Epoch 64/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6647\n",
      "Epoch 65/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7654\n",
      "Epoch 66/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6436\n",
      "Epoch 67/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6885\n",
      "Epoch 68/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7293\n",
      "Epoch 69/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8126\n",
      "Epoch 70/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6880\n",
      "Epoch 71/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6515\n",
      "Epoch 72/200\n",
      "3/3 [==============================] - 0s 953us/step - loss: 0.6792\n",
      "Epoch 73/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6129\n",
      "Epoch 74/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7861\n",
      "Epoch 75/200\n",
      "3/3 [==============================] - 0s 985us/step - loss: 0.7032\n",
      "Epoch 76/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6835\n",
      "Epoch 77/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7260\n",
      "Epoch 78/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6836\n",
      "Epoch 79/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6773\n",
      "Epoch 80/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7160\n",
      "Epoch 81/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7682\n",
      "Epoch 82/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7046\n",
      "Epoch 83/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6573\n",
      "Epoch 84/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8183\n",
      "Epoch 85/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7065\n",
      "Epoch 86/200\n",
      "3/3 [==============================] - 0s 957us/step - loss: 0.7185\n",
      "Epoch 87/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6997\n",
      "Epoch 88/200\n",
      "3/3 [==============================] - 0s 988us/step - loss: 0.7128\n",
      "Epoch 89/200\n",
      "3/3 [==============================] - 0s 975us/step - loss: 0.7269\n",
      "Epoch 90/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7296\n",
      "Epoch 91/200\n",
      "3/3 [==============================] - 0s 927us/step - loss: 0.7148\n",
      "Epoch 92/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7018\n",
      "Epoch 93/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6607\n",
      "Epoch 94/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7067\n",
      "Epoch 95/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7221\n",
      "Epoch 96/200\n",
      "3/3 [==============================] - 0s 921us/step - loss: 0.7636\n",
      "Epoch 97/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7617\n",
      "Epoch 98/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7314\n",
      "Epoch 99/200\n",
      "3/3 [==============================] - 0s 823us/step - loss: 0.7434\n",
      "Epoch 100/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7052\n",
      "Epoch 101/200\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6595\n",
      "Epoch 102/200\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.7708\n",
      "Epoch 103/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.6250\n",
      "Epoch 104/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7231\n",
      "Epoch 105/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7835\n",
      "Epoch 106/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6713\n",
      "Epoch 107/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7152\n",
      "Epoch 108/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.7054\n",
      "Epoch 109/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6841\n",
      "Epoch 110/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.6701\n",
      "Epoch 111/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.6802\n",
      "Epoch 112/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7113\n",
      "Epoch 113/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7045\n",
      "Epoch 114/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6446\n",
      "Epoch 115/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7173\n",
      "Epoch 116/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7580\n",
      "Epoch 117/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6602\n",
      "Epoch 118/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7045\n",
      "Epoch 119/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7195\n",
      "Epoch 120/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7254\n",
      "Epoch 121/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7569\n",
      "Epoch 122/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6337\n",
      "Epoch 123/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6843\n",
      "Epoch 124/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7488\n",
      "Epoch 125/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8009\n",
      "Epoch 126/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7578\n",
      "Epoch 127/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6652\n",
      "Epoch 128/200\n",
      "3/3 [==============================] - 0s 942us/step - loss: 0.8037\n",
      "Epoch 129/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6866\n",
      "Epoch 130/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7448\n",
      "Epoch 131/200\n",
      "3/3 [==============================] - 0s 876us/step - loss: 0.7016\n",
      "Epoch 132/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7635\n",
      "Epoch 133/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7089\n",
      "Epoch 134/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6648\n",
      "Epoch 135/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8207\n",
      "Epoch 136/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7076\n",
      "Epoch 137/200\n",
      "3/3 [==============================] - 0s 927us/step - loss: 0.6658\n",
      "Epoch 138/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6898\n",
      "Epoch 139/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6420\n",
      "Epoch 140/200\n",
      "3/3 [==============================] - 0s 915us/step - loss: 0.7036\n",
      "Epoch 141/200\n",
      "3/3 [==============================] - 0s 877us/step - loss: 0.7050\n",
      "Epoch 142/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6588\n",
      "Epoch 143/200\n",
      "3/3 [==============================] - 0s 993us/step - loss: 0.6949\n",
      "Epoch 144/200\n",
      "3/3 [==============================] - 0s 815us/step - loss: 0.6750\n",
      "Epoch 145/200\n",
      "3/3 [==============================] - 0s 975us/step - loss: 0.7121\n",
      "Epoch 146/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6637\n",
      "Epoch 147/200\n",
      "3/3 [==============================] - 0s 894us/step - loss: 0.6429\n",
      "Epoch 148/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5944\n",
      "Epoch 149/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6758\n",
      "Epoch 150/200\n",
      "3/3 [==============================] - 0s 948us/step - loss: 0.7195\n",
      "Epoch 151/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7205\n",
      "Epoch 152/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7756\n",
      "Epoch 153/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6497\n",
      "Epoch 154/200\n",
      "3/3 [==============================] - 0s 925us/step - loss: 0.6706\n",
      "Epoch 155/200\n",
      "3/3 [==============================] - 0s 853us/step - loss: 0.7069\n",
      "Epoch 156/200\n",
      "3/3 [==============================] - 0s 934us/step - loss: 0.6221\n",
      "Epoch 157/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7945\n",
      "Epoch 158/200\n",
      "3/3 [==============================] - 0s 870us/step - loss: 0.6261\n",
      "Epoch 159/200\n",
      "3/3 [==============================] - 0s 823us/step - loss: 0.6515\n",
      "Epoch 160/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6099\n",
      "Epoch 161/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5805\n",
      "Epoch 162/200\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6286\n",
      "Epoch 163/200\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.6465\n",
      "Epoch 164/200\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.6975\n",
      "Epoch 165/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.6865\n",
      "Epoch 166/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5845\n",
      "Epoch 167/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7512\n",
      "Epoch 168/200\n",
      "3/3 [==============================] - 0s 955us/step - loss: 0.6552\n",
      "Epoch 169/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6747\n",
      "Epoch 170/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6301\n",
      "Epoch 171/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6019\n",
      "Epoch 172/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6772\n",
      "Epoch 173/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7183\n",
      "Epoch 174/200\n",
      "3/3 [==============================] - 0s 972us/step - loss: 0.5966\n",
      "Epoch 175/200\n",
      "3/3 [==============================] - 0s 988us/step - loss: 0.6901\n",
      "Epoch 176/200\n",
      "3/3 [==============================] - 0s 986us/step - loss: 0.6867\n",
      "Epoch 177/200\n",
      "3/3 [==============================] - 0s 974us/step - loss: 0.6956\n",
      "Epoch 178/200\n",
      "3/3 [==============================] - 0s 940us/step - loss: 0.6702\n",
      "Epoch 179/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6425\n",
      "Epoch 180/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6333\n",
      "Epoch 181/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6739\n",
      "Epoch 182/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6796\n",
      "Epoch 183/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6829\n",
      "Epoch 184/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6978\n",
      "Epoch 185/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6355\n",
      "Epoch 186/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5923\n",
      "Epoch 187/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6933\n",
      "Epoch 188/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6783\n",
      "Epoch 189/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6238\n",
      "Epoch 190/200\n",
      "3/3 [==============================] - 0s 921us/step - loss: 0.7191\n",
      "Epoch 191/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6177\n",
      "Epoch 192/200\n",
      "3/3 [==============================] - 0s 845us/step - loss: 0.6329\n",
      "Epoch 193/200\n",
      "3/3 [==============================] - 0s 869us/step - loss: 0.6751\n",
      "Epoch 194/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6797\n",
      "Epoch 195/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6310\n",
      "Epoch 196/200\n",
      "3/3 [==============================] - 0s 909us/step - loss: 0.5777\n",
      "Epoch 197/200\n",
      "3/3 [==============================] - 0s 852us/step - loss: 0.7149\n",
      "Epoch 198/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7065\n",
      "Epoch 199/200\n",
      "3/3 [==============================] - 0s 891us/step - loss: 0.6834\n",
      "Epoch 200/200\n",
      "3/3 [==============================] - 0s 985us/step - loss: 0.7137\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Epoch 1/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8503\n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.9869\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8396\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 0s 990us/step - loss: 0.8637\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8165\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.9135\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8490\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 0s 920us/step - loss: 0.8904\n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7580\n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9352\n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7421\n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.0078\n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.8334\n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.8810\n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.8464\n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8718\n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8334\n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.8249\n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.8012\n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7536\n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7685\n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9305\n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8269\n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7603\n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8447\n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7986\n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7609\n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7009\n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7521\n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6723\n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7423\n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8114\n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7468\n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7939\n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8468\n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7378\n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7108\n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7367\n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7349\n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8396\n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7292\n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7569\n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8310\n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6945\n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8430\n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 0s 926us/step - loss: 0.7887\n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7342\n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7186\n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7758\n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6956\n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7633\n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 0s 975us/step - loss: 0.7096\n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 0s 898us/step - loss: 0.8267\n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 0s 980us/step - loss: 0.8139\n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 0s 937us/step - loss: 0.7972\n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 0s 995us/step - loss: 0.7751\n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7188\n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 0s 939us/step - loss: 0.7637\n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 0s 910us/step - loss: 0.8281\n",
      "Epoch 60/200\n",
      "3/3 [==============================] - 0s 925us/step - loss: 0.7315\n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 0s 958us/step - loss: 0.7132\n",
      "Epoch 62/200\n",
      "3/3 [==============================] - 0s 915us/step - loss: 0.7912\n",
      "Epoch 63/200\n",
      "3/3 [==============================] - 0s 982us/step - loss: 0.8352\n",
      "Epoch 64/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6354\n",
      "Epoch 65/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7509\n",
      "Epoch 66/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8317\n",
      "Epoch 67/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7702\n",
      "Epoch 68/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7864\n",
      "Epoch 69/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7446\n",
      "Epoch 70/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7381\n",
      "Epoch 71/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7298\n",
      "Epoch 72/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7372\n",
      "Epoch 73/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7488\n",
      "Epoch 74/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7627\n",
      "Epoch 75/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6991\n",
      "Epoch 76/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8070\n",
      "Epoch 77/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6750\n",
      "Epoch 78/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6470\n",
      "Epoch 79/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6307\n",
      "Epoch 80/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7795\n",
      "Epoch 81/200\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.6599\n",
      "Epoch 82/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.6811\n",
      "Epoch 83/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7445\n",
      "Epoch 84/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7774\n",
      "Epoch 85/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7586\n",
      "Epoch 86/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6427\n",
      "Epoch 87/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6500\n",
      "Epoch 88/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7438\n",
      "Epoch 89/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.7462\n",
      "Epoch 90/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7058\n",
      "Epoch 91/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7599\n",
      "Epoch 92/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7449\n",
      "Epoch 93/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7658\n",
      "Epoch 94/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7224\n",
      "Epoch 95/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7481\n",
      "Epoch 96/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6803\n",
      "Epoch 97/200\n",
      "3/3 [==============================] - 0s 986us/step - loss: 0.7583\n",
      "Epoch 98/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6757\n",
      "Epoch 99/200\n",
      "3/3 [==============================] - 0s 939us/step - loss: 0.8024\n",
      "Epoch 100/200\n",
      "3/3 [==============================] - 0s 926us/step - loss: 0.7244\n",
      "Epoch 101/200\n",
      "3/3 [==============================] - 0s 994us/step - loss: 0.7398\n",
      "Epoch 102/200\n",
      "3/3 [==============================] - 0s 972us/step - loss: 0.8485\n",
      "Epoch 103/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7261\n",
      "Epoch 104/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6319\n",
      "Epoch 105/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7616\n",
      "Epoch 106/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6976\n",
      "Epoch 107/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6831\n",
      "Epoch 108/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7716\n",
      "Epoch 109/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7027\n",
      "Epoch 110/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7367\n",
      "Epoch 111/200\n",
      "3/3 [==============================] - 0s 903us/step - loss: 0.7589\n",
      "Epoch 112/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8235\n",
      "Epoch 113/200\n",
      "3/3 [==============================] - 0s 880us/step - loss: 0.7370\n",
      "Epoch 114/200\n",
      "3/3 [==============================] - 0s 782us/step - loss: 0.6914\n",
      "Epoch 115/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7004\n",
      "Epoch 116/200\n",
      "3/3 [==============================] - 0s 957us/step - loss: 0.7516\n",
      "Epoch 117/200\n",
      "3/3 [==============================] - 0s 762us/step - loss: 0.7263\n",
      "Epoch 118/200\n",
      "3/3 [==============================] - 0s 826us/step - loss: 0.6908\n",
      "Epoch 119/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6493\n",
      "Epoch 120/200\n",
      "3/3 [==============================] - 0s 835us/step - loss: 0.7366\n",
      "Epoch 121/200\n",
      "3/3 [==============================] - 0s 876us/step - loss: 0.6844\n",
      "Epoch 122/200\n",
      "3/3 [==============================] - 0s 935us/step - loss: 0.6871\n",
      "Epoch 123/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6735\n",
      "Epoch 124/200\n",
      "3/3 [==============================] - 0s 839us/step - loss: 0.7427\n",
      "Epoch 125/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7369\n",
      "Epoch 126/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6550\n",
      "Epoch 127/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7054\n",
      "Epoch 128/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7320\n",
      "Epoch 129/200\n",
      "3/3 [==============================] - 0s 883us/step - loss: 0.6467\n",
      "Epoch 130/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8226\n",
      "Epoch 131/200\n",
      "3/3 [==============================] - 0s 889us/step - loss: 0.7122\n",
      "Epoch 132/200\n",
      "3/3 [==============================] - 0s 840us/step - loss: 0.7538\n",
      "Epoch 133/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7165\n",
      "Epoch 134/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6801\n",
      "Epoch 135/200\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.7490\n",
      "Epoch 136/200\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.7248\n",
      "Epoch 137/200\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.7085\n",
      "Epoch 138/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7354\n",
      "Epoch 139/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7059\n",
      "Epoch 140/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6885\n",
      "Epoch 141/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7051\n",
      "Epoch 142/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7432\n",
      "Epoch 143/200\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.7093\n",
      "Epoch 144/200\n",
      "3/3 [==============================] - 0s 937us/step - loss: 0.6880\n",
      "Epoch 145/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6859\n",
      "Epoch 146/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6370\n",
      "Epoch 147/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7560\n",
      "Epoch 148/200\n",
      "3/3 [==============================] - 0s 941us/step - loss: 0.7400\n",
      "Epoch 149/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6940\n",
      "Epoch 150/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6616\n",
      "Epoch 151/200\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.7094\n",
      "Epoch 152/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.6445\n",
      "Epoch 153/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7246\n",
      "Epoch 154/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7056\n",
      "Epoch 155/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7150\n",
      "Epoch 156/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6767\n",
      "Epoch 157/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6968\n",
      "Epoch 158/200\n",
      "3/3 [==============================] - 0s 909us/step - loss: 0.7040\n",
      "Epoch 159/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8198\n",
      "Epoch 160/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6402\n",
      "Epoch 161/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6954\n",
      "Epoch 162/200\n",
      "3/3 [==============================] - 0s 958us/step - loss: 0.7271\n",
      "Epoch 163/200\n",
      "3/3 [==============================] - 0s 808us/step - loss: 0.7058\n",
      "Epoch 164/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6731\n",
      "Epoch 165/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6627\n",
      "Epoch 166/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6803\n",
      "Epoch 167/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7321\n",
      "Epoch 168/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6760\n",
      "Epoch 169/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6654\n",
      "Epoch 170/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7188\n",
      "Epoch 171/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6660\n",
      "Epoch 172/200\n",
      "3/3 [==============================] - 0s 876us/step - loss: 0.6893\n",
      "Epoch 173/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7474\n",
      "Epoch 174/200\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.6889\n",
      "Epoch 175/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7110\n",
      "Epoch 176/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6522\n",
      "Epoch 177/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6864\n",
      "Epoch 178/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6671\n",
      "Epoch 179/200\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.6965\n",
      "Epoch 180/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6470\n",
      "Epoch 181/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6749\n",
      "Epoch 182/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7534\n",
      "Epoch 183/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.6157\n",
      "Epoch 184/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.7022\n",
      "Epoch 185/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.6908\n",
      "Epoch 186/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6930\n",
      "Epoch 187/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6165\n",
      "Epoch 188/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6556\n",
      "Epoch 189/200\n",
      "3/3 [==============================] - 0s 973us/step - loss: 0.6668\n",
      "Epoch 190/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6984\n",
      "Epoch 191/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7069\n",
      "Epoch 192/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6508\n",
      "Epoch 193/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7072\n",
      "Epoch 194/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6350\n",
      "Epoch 195/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6554\n",
      "Epoch 196/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7092\n",
      "Epoch 197/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6930\n",
      "Epoch 198/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5880\n",
      "Epoch 199/200\n",
      "3/3 [==============================] - 0s 971us/step - loss: 0.7023\n",
      "Epoch 200/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6943\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "Epoch 1/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.9960\n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.9684\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.9559\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.9244\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.9249\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.8678\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9196\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.0297\n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7760\n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7240\n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 0s 972us/step - loss: 0.8436\n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.9030\n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8513\n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.9030\n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8706\n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8085\n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8442\n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 0s 992us/step - loss: 0.8305\n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 0s 882us/step - loss: 0.8579\n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8028\n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 0s 970us/step - loss: 0.7742\n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.9433\n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8967\n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.9659\n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6494\n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.8280\n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.7958\n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8482\n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7622\n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7578\n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.8277\n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.7286\n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.8734\n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.6604\n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.8321\n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7764\n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.7854\n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.8032\n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8730\n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8009\n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8634\n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7791\n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7375\n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8324\n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8106\n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 0s 976us/step - loss: 0.8012\n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 0s 938us/step - loss: 0.7941\n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.8001\n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7869\n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7724\n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 0s 946us/step - loss: 0.8295\n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6709\n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8012\n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6974\n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 0s 887us/step - loss: 0.7427\n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 0s 959us/step - loss: 0.7299\n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7110\n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7735\n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7821\n",
      "Epoch 60/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7164\n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8532\n",
      "Epoch 62/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7307\n",
      "Epoch 63/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7898\n",
      "Epoch 64/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7679\n",
      "Epoch 65/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6580\n",
      "Epoch 66/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7807\n",
      "Epoch 67/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7751\n",
      "Epoch 68/200\n",
      "3/3 [==============================] - 0s 849us/step - loss: 0.7314\n",
      "Epoch 69/200\n",
      "3/3 [==============================] - 0s 844us/step - loss: 0.7201\n",
      "Epoch 70/200\n",
      "3/3 [==============================] - 0s 906us/step - loss: 0.6971\n",
      "Epoch 71/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6730\n",
      "Epoch 72/200\n",
      "3/3 [==============================] - 0s 897us/step - loss: 0.6642\n",
      "Epoch 73/200\n",
      "3/3 [==============================] - 0s 791us/step - loss: 0.7914\n",
      "Epoch 74/200\n",
      "3/3 [==============================] - 0s 983us/step - loss: 0.7321\n",
      "Epoch 75/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7851\n",
      "Epoch 76/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7241\n",
      "Epoch 77/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7053\n",
      "Epoch 78/200\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.7248\n",
      "Epoch 79/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7348\n",
      "Epoch 80/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6953\n",
      "Epoch 81/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6991\n",
      "Epoch 82/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7304\n",
      "Epoch 83/200\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.7890\n",
      "Epoch 84/200\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.7084\n",
      "Epoch 85/200\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.7286\n",
      "Epoch 86/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7137\n",
      "Epoch 87/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7859\n",
      "Epoch 88/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6371\n",
      "Epoch 89/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6968\n",
      "Epoch 90/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8216\n",
      "Epoch 91/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7123\n",
      "Epoch 92/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6997\n",
      "Epoch 93/200\n",
      "3/3 [==============================] - 0s 963us/step - loss: 0.7265\n",
      "Epoch 94/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5447\n",
      "Epoch 95/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7255\n",
      "Epoch 96/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6728\n",
      "Epoch 97/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7223\n",
      "Epoch 98/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6827\n",
      "Epoch 99/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.8504\n",
      "Epoch 100/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7197\n",
      "Epoch 101/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6920\n",
      "Epoch 102/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6524\n",
      "Epoch 103/200\n",
      "3/3 [==============================] - 0s 975us/step - loss: 0.6808\n",
      "Epoch 104/200\n",
      "3/3 [==============================] - 0s 929us/step - loss: 0.7299\n",
      "Epoch 105/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6025\n",
      "Epoch 106/200\n",
      "3/3 [==============================] - 0s 914us/step - loss: 0.6902\n",
      "Epoch 107/200\n",
      "3/3 [==============================] - 0s 963us/step - loss: 0.6952\n",
      "Epoch 108/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8075\n",
      "Epoch 109/200\n",
      "3/3 [==============================] - 0s 889us/step - loss: 0.6507\n",
      "Epoch 110/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6701\n",
      "Epoch 111/200\n",
      "3/3 [==============================] - 0s 984us/step - loss: 0.7302\n",
      "Epoch 112/200\n",
      "3/3 [==============================] - 0s 913us/step - loss: 0.6719\n",
      "Epoch 113/200\n",
      "3/3 [==============================] - 0s 800us/step - loss: 0.7112\n",
      "Epoch 114/200\n",
      "3/3 [==============================] - 0s 938us/step - loss: 0.6932\n",
      "Epoch 115/200\n",
      "3/3 [==============================] - 0s 804us/step - loss: 0.6945\n",
      "Epoch 116/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6651\n",
      "Epoch 117/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6952\n",
      "Epoch 118/200\n",
      "3/3 [==============================] - 0s 970us/step - loss: 0.7089\n",
      "Epoch 119/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6852\n",
      "Epoch 120/200\n",
      "3/3 [==============================] - 0s 869us/step - loss: 0.6443\n",
      "Epoch 121/200\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.7285\n",
      "Epoch 122/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7027\n",
      "Epoch 123/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6587\n",
      "Epoch 124/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7221\n",
      "Epoch 125/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6629\n",
      "Epoch 126/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6013\n",
      "Epoch 127/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6818\n",
      "Epoch 128/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6683\n",
      "Epoch 129/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6837\n",
      "Epoch 130/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6784\n",
      "Epoch 131/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7643\n",
      "Epoch 132/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6862\n",
      "Epoch 133/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6816\n",
      "Epoch 134/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6512\n",
      "Epoch 135/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6835\n",
      "Epoch 136/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7470\n",
      "Epoch 137/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6855\n",
      "Epoch 138/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6793\n",
      "Epoch 139/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7359\n",
      "Epoch 140/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6418\n",
      "Epoch 141/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7039\n",
      "Epoch 142/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7670\n",
      "Epoch 143/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7068\n",
      "Epoch 144/200\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.6768\n",
      "Epoch 145/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6282\n",
      "Epoch 146/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6458\n",
      "Epoch 147/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6508\n",
      "Epoch 148/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6198\n",
      "Epoch 149/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7214\n",
      "Epoch 150/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7184\n",
      "Epoch 151/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7243\n",
      "Epoch 152/200\n",
      "3/3 [==============================] - 0s 937us/step - loss: 0.7037\n",
      "Epoch 153/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6481\n",
      "Epoch 154/200\n",
      "3/3 [==============================] - 0s 915us/step - loss: 0.6774\n",
      "Epoch 155/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7692\n",
      "Epoch 156/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6662\n",
      "Epoch 157/200\n",
      "3/3 [==============================] - 0s 985us/step - loss: 0.7375\n",
      "Epoch 158/200\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.6532\n",
      "Epoch 159/200\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6775\n",
      "Epoch 160/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7154\n",
      "Epoch 161/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7054\n",
      "Epoch 162/200\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.6989\n",
      "Epoch 163/200\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.7210\n",
      "Epoch 164/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7490\n",
      "Epoch 165/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7372\n",
      "Epoch 166/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6473\n",
      "Epoch 167/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6776\n",
      "Epoch 168/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7090\n",
      "Epoch 169/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6302\n",
      "Epoch 170/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5854\n",
      "Epoch 171/200\n",
      "3/3 [==============================] - 0s 965us/step - loss: 0.7142\n",
      "Epoch 172/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6713\n",
      "Epoch 173/200\n",
      "3/3 [==============================] - 0s 955us/step - loss: 0.6637\n",
      "Epoch 174/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6848\n",
      "Epoch 175/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6779\n",
      "Epoch 176/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6755\n",
      "Epoch 177/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6661\n",
      "Epoch 178/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6728\n",
      "Epoch 179/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6279\n",
      "Epoch 180/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7286\n",
      "Epoch 181/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6565\n",
      "Epoch 182/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6463\n",
      "Epoch 183/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6852\n",
      "Epoch 184/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6638\n",
      "Epoch 185/200\n",
      "3/3 [==============================] - 0s 929us/step - loss: 0.6674\n",
      "Epoch 186/200\n",
      "3/3 [==============================] - 0s 825us/step - loss: 0.6981\n",
      "Epoch 187/200\n",
      "3/3 [==============================] - 0s 871us/step - loss: 0.6548\n",
      "Epoch 188/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7200\n",
      "Epoch 189/200\n",
      "3/3 [==============================] - 0s 917us/step - loss: 0.6668\n",
      "Epoch 190/200\n",
      "3/3 [==============================] - 0s 767us/step - loss: 0.7095\n",
      "Epoch 191/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6696\n",
      "Epoch 192/200\n",
      "3/3 [==============================] - 0s 899us/step - loss: 0.6065\n",
      "Epoch 193/200\n",
      "3/3 [==============================] - 0s 981us/step - loss: 0.6506\n",
      "Epoch 194/200\n",
      "3/3 [==============================] - 0s 854us/step - loss: 0.6320\n",
      "Epoch 195/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6671\n",
      "Epoch 196/200\n",
      "3/3 [==============================] - 0s 935us/step - loss: 0.7078\n",
      "Epoch 197/200\n",
      "3/3 [==============================] - 0s 780us/step - loss: 0.6504\n",
      "Epoch 198/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6534\n",
      "Epoch 199/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6819\n",
      "Epoch 200/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6225\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "Epoch 1/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.0066\n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.9333\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.8383\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8367\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8882\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8686\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.9421\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.8189\n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8459\n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.9269\n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 0s 990us/step - loss: 0.7462\n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 0s 963us/step - loss: 1.0963\n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 0s 869us/step - loss: 1.0241\n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8380\n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8537\n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8291\n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7709\n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 0s 967us/step - loss: 0.8580\n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7688\n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8816\n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 0s 985us/step - loss: 0.8899\n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.9338\n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 1.0138\n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7121\n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.9714\n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.8089\n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.8193\n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.8200\n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.7754\n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.8218\n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 1.0031\n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 1.0337\n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8809\n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7680\n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7593\n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8116\n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6467\n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8808\n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.8282\n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7882\n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.7555\n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 0s 932us/step - loss: 0.8030\n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 0s 947us/step - loss: 0.6432\n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7551\n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8067\n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 0s 977us/step - loss: 0.7778\n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 0s 986us/step - loss: 0.7555\n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 0s 952us/step - loss: 0.6531\n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 0s 992us/step - loss: 0.7983\n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7366\n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7805\n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7123\n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.8993\n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7642\n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7246\n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8234\n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6722\n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7155\n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7275\n",
      "Epoch 60/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.8577\n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.9060\n",
      "Epoch 62/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7241\n",
      "Epoch 63/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7843\n",
      "Epoch 64/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7525\n",
      "Epoch 65/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7795\n",
      "Epoch 66/200\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.8051\n",
      "Epoch 67/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.7127\n",
      "Epoch 68/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7351\n",
      "Epoch 69/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.8242\n",
      "Epoch 70/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.8096\n",
      "Epoch 71/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7331\n",
      "Epoch 72/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7041\n",
      "Epoch 73/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7254\n",
      "Epoch 74/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.8121\n",
      "Epoch 75/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.8141\n",
      "Epoch 76/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7441\n",
      "Epoch 77/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6926\n",
      "Epoch 78/200\n",
      "3/3 [==============================] - 0s 992us/step - loss: 0.6965\n",
      "Epoch 79/200\n",
      "3/3 [==============================] - 0s 983us/step - loss: 0.7595\n",
      "Epoch 80/200\n",
      "3/3 [==============================] - 0s 971us/step - loss: 0.7306\n",
      "Epoch 81/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8061\n",
      "Epoch 82/200\n",
      "3/3 [==============================] - 0s 957us/step - loss: 0.7827\n",
      "Epoch 83/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7038\n",
      "Epoch 84/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7205\n",
      "Epoch 85/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7333\n",
      "Epoch 86/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7813\n",
      "Epoch 87/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.8060\n",
      "Epoch 88/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7219\n",
      "Epoch 89/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7622\n",
      "Epoch 90/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8007\n",
      "Epoch 91/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7388\n",
      "Epoch 92/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8124\n",
      "Epoch 93/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7991\n",
      "Epoch 94/200\n",
      "3/3 [==============================] - 0s 957us/step - loss: 0.6519\n",
      "Epoch 95/200\n",
      "3/3 [==============================] - 0s 832us/step - loss: 0.7249\n",
      "Epoch 96/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7307\n",
      "Epoch 97/200\n",
      "3/3 [==============================] - 0s 826us/step - loss: 0.8078\n",
      "Epoch 98/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7494\n",
      "Epoch 99/200\n",
      "3/3 [==============================] - 0s 909us/step - loss: 0.7169\n",
      "Epoch 100/200\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.6862\n",
      "Epoch 101/200\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.7763\n",
      "Epoch 102/200\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.8190\n",
      "Epoch 103/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7468\n",
      "Epoch 104/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7181\n",
      "Epoch 105/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7749\n",
      "Epoch 106/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6878\n",
      "Epoch 107/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7217\n",
      "Epoch 108/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7663\n",
      "Epoch 109/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8062\n",
      "Epoch 110/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7801\n",
      "Epoch 111/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6777\n",
      "Epoch 112/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7531\n",
      "Epoch 113/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7464\n",
      "Epoch 114/200\n",
      "3/3 [==============================] - 0s 991us/step - loss: 0.7131\n",
      "Epoch 115/200\n",
      "3/3 [==============================] - 0s 963us/step - loss: 0.7014\n",
      "Epoch 116/200\n",
      "3/3 [==============================] - 0s 986us/step - loss: 0.7826\n",
      "Epoch 117/200\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.7532\n",
      "Epoch 118/200\n",
      "3/3 [==============================] - 0s 931us/step - loss: 0.8014\n",
      "Epoch 119/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7602\n",
      "Epoch 120/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7308\n",
      "Epoch 121/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6963\n",
      "Epoch 122/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7340\n",
      "Epoch 123/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7318\n",
      "Epoch 124/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7276\n",
      "Epoch 125/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7068\n",
      "Epoch 126/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6889\n",
      "Epoch 127/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7531\n",
      "Epoch 128/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7040\n",
      "Epoch 129/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7235\n",
      "Epoch 130/200\n",
      "3/3 [==============================] - 0s 877us/step - loss: 0.6520\n",
      "Epoch 131/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6989\n",
      "Epoch 132/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7305\n",
      "Epoch 133/200\n",
      "3/3 [==============================] - 0s 885us/step - loss: 0.7478\n",
      "Epoch 134/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6920\n",
      "Epoch 135/200\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6830\n",
      "Epoch 136/200\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.6666\n",
      "Epoch 137/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7363\n",
      "Epoch 138/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7138\n",
      "Epoch 139/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6825\n",
      "Epoch 140/200\n",
      "3/3 [==============================] - 0s 980us/step - loss: 0.7525\n",
      "Epoch 141/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7338\n",
      "Epoch 142/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6941\n",
      "Epoch 143/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6911\n",
      "Epoch 144/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7282\n",
      "Epoch 145/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6928\n",
      "Epoch 146/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7995\n",
      "Epoch 147/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8419\n",
      "Epoch 148/200\n",
      "3/3 [==============================] - 0s 992us/step - loss: 0.7214\n",
      "Epoch 149/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7520\n",
      "Epoch 150/200\n",
      "3/3 [==============================] - 0s 970us/step - loss: 0.7354\n",
      "Epoch 151/200\n",
      "3/3 [==============================] - 0s 966us/step - loss: 0.7297\n",
      "Epoch 152/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7345\n",
      "Epoch 153/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7070\n",
      "Epoch 154/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7563\n",
      "Epoch 155/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7072\n",
      "Epoch 156/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6538\n",
      "Epoch 157/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7085\n",
      "Epoch 158/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7653\n",
      "Epoch 159/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7618\n",
      "Epoch 160/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.8153\n",
      "Epoch 161/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7052\n",
      "Epoch 162/200\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.7520\n",
      "Epoch 163/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6918\n",
      "Epoch 164/200\n",
      "3/3 [==============================] - 0s 916us/step - loss: 0.6789\n",
      "Epoch 165/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7542\n",
      "Epoch 166/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6691\n",
      "Epoch 167/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7614\n",
      "Epoch 168/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.7216\n",
      "Epoch 169/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.6765\n",
      "Epoch 170/200\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.7441\n",
      "Epoch 171/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.7074\n",
      "Epoch 172/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.6668\n",
      "Epoch 173/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.7428\n",
      "Epoch 174/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7110\n",
      "Epoch 175/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6733\n",
      "Epoch 176/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6845\n",
      "Epoch 177/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6496\n",
      "Epoch 178/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7217\n",
      "Epoch 179/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7112\n",
      "Epoch 180/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6950\n",
      "Epoch 181/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7094\n",
      "Epoch 182/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7951\n",
      "Epoch 183/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6795\n",
      "Epoch 184/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7005\n",
      "Epoch 185/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6872\n",
      "Epoch 186/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7083\n",
      "Epoch 187/200\n",
      "3/3 [==============================] - 0s 938us/step - loss: 0.7769\n",
      "Epoch 188/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7785\n",
      "Epoch 189/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6909\n",
      "Epoch 190/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6806\n",
      "Epoch 191/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.7583\n",
      "Epoch 192/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6468\n",
      "Epoch 193/200\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.7755\n",
      "Epoch 194/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6760\n",
      "Epoch 195/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6966\n",
      "Epoch 196/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.7148\n",
      "Epoch 197/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.7554\n",
      "Epoch 198/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6932\n",
      "Epoch 199/200\n",
      "3/3 [==============================] - 0s 914us/step - loss: 0.7292\n",
      "Epoch 200/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6171\n",
      "1/1 [==============================] - 0s 110ms/step\n"
     ]
    }
   ],
   "source": [
    "y_preds_final = []\n",
    "for j in range(5):\n",
    "    #f_training_X_img = df_training_X[[str(j) for j in range(2048*j, 2048 + 2048*j)]]\n",
    "    #df_training_y_img = downsampled_df['price_bucket'].apply(convert_price)\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(df_training_X_img, df_training_y_img, test_size=.25, random_state=22)\n",
    "    X_train_PCA_meta, X_test_PCA_meta = run_PCA(X_train[[str(j) for j in range(2048*j, 2048 + 2048*j)]], X_test[[str(j) for j in range(2048*j, 2048 + 2048*j)]], use_all=False, num_components=10)\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(32, input_dim = X_train_PCA_meta.shape[1], activation='tanh'))\n",
    "    model.add(Dropout(0.5)) \n",
    "    model.add(Dense(16, input_dim = X_train_PCA_meta.shape[1], activation='tanh'))\n",
    "    model.add(Dropout(0.5)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(8, input_dim = X_train_PCA_meta.shape[1], activation='tanh'))\n",
    "    model.add(Dropout(0.5)) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    model.fit(X_train_PCA_meta, y_train, epochs=200)\n",
    "\n",
    "    y_preds_MLP = model.predict(X_test_PCA_meta).squeeze()\n",
    "    y_preds_MLP = (y_preds_MLP > 0.5).astype(int)\n",
    "    y_preds_final.append(y_preds_MLP)\n",
    "\n",
    "meta_matrix = pd.DataFrame(y_preds_final)\n",
    "y_preds_META_MODEL = [meta_matrix[i].mode().squeeze() for i in range(25)]\n",
    "error_NN_meta = accuracy_score(y_test, y_preds_META_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_NN_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../app/random_forest_model_0.pkl'\n",
    "\n",
    "with open(model_path, 'rb') as file:\n",
    "    test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k_/y5wzmsx1483f9z9t9tjtdj1m0000gn/T/ipykernel_15381/3265576334.py:1: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  stats.mode([1,2,2])[0][0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.mode([1,2,2])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(features):\n",
    "    predictions = []\n",
    "    for j in range(5):\n",
    "        model_path = f'../app/random_forest_model_{j}.pkl'\n",
    "\n",
    "        with open(model_path, 'rb') as file:\n",
    "            model = pickle.load(file)\n",
    "\n",
    "        prediction = model.predict(features[:, 2048*j: 2048 + 2048*j])\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return stats.mode(predictions)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(url_arr):\n",
    "    images = []\n",
    "    print(url_arr)\n",
    "\n",
    "    for url in url_arr:\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        images.append(img)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(url_arr):\n",
    "    images = []\n",
    "\n",
    "    for url in url_arr:\n",
    "        img = Image.open(url)\n",
    "        images.append(img)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_arr = ['test2.jpeg' for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test2.jpeg', 'test2.jpeg', 'test2.jpeg', 'test2.jpeg', 'test2.jpeg']\n"
     ]
    }
   ],
   "source": [
    "test = load_image(url_arr=url_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_extract_features(img_arr):\n",
    "    features = []\n",
    "\n",
    "    resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "    for img in img_arr:\n",
    "        img_array = img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "\n",
    "        features_img = resnet.predict(img_array)\n",
    "\n",
    "        features.extend(features_img[0])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 669ms/step\n",
      "1/1 [==============================] - 0s 239ms/step\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "1/1 [==============================] - 0s 307ms/step\n",
      "1/1 [==============================] - 0s 277ms/step\n"
     ]
    }
   ],
   "source": [
    "feat = prepare_extract_features(img_arr=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.31222063,\n",
       " 0.17221065,\n",
       " 1.3446822,\n",
       " 0.010240825,\n",
       " 0.43195227,\n",
       " 0.031182546,\n",
       " 0.0155016845,\n",
       " 0.029019369,\n",
       " 0.42756066,\n",
       " 0.014874036,\n",
       " 0.32372195,\n",
       " 0.18074471,\n",
       " 0.46788847,\n",
       " 0.7044107,\n",
       " 0.14528532,\n",
       " 0.028149676,\n",
       " 0.01974666,\n",
       " 0.0028312386,\n",
       " 0.47781754,\n",
       " 0.1713444,\n",
       " 0.11797799,\n",
       " 0.06346274,\n",
       " 0.2503104,\n",
       " 0.40074503,\n",
       " 0.02807137,\n",
       " 1.3308756,\n",
       " 0.27151337,\n",
       " 1.1274112,\n",
       " 0.008978986,\n",
       " 4.0886617,\n",
       " 0.017905775,\n",
       " 0.30960453,\n",
       " 0.48425302,\n",
       " 0.054721788,\n",
       " 0.13850228,\n",
       " 0.047077816,\n",
       " 0.18923755,\n",
       " 0.0889336,\n",
       " 0.23684281,\n",
       " 0.063227616,\n",
       " 0.03030495,\n",
       " 0.413069,\n",
       " 0.16253786,\n",
       " 0.12764983,\n",
       " 0.20045128,\n",
       " 0.040668733,\n",
       " 0.16858797,\n",
       " 0.47242272,\n",
       " 0.004032733,\n",
       " 0.3395215,\n",
       " 0.7512693,\n",
       " 0.2626947,\n",
       " 0.029037146,\n",
       " 0.06817128,\n",
       " 0.63301104,\n",
       " 0.118644856,\n",
       " 0.06369893,\n",
       " 0.5486805,\n",
       " 0.10635554,\n",
       " 0.26945332,\n",
       " 0.37883624,\n",
       " 0.10667766,\n",
       " 0.882942,\n",
       " 0.03948549,\n",
       " 0.032641944,\n",
       " 0.9799549,\n",
       " 0.0206137,\n",
       " 0.045486562,\n",
       " 0.069370136,\n",
       " 0.3966892,\n",
       " 0.15802743,\n",
       " 0.15688881,\n",
       " 1.0275323,\n",
       " 0.58452004,\n",
       " 0.09528443,\n",
       " 0.17645091,\n",
       " 0.32551718,\n",
       " 0.07765616,\n",
       " 0.6609985,\n",
       " 0.12111564,\n",
       " 0.029965125,\n",
       " 0.046332195,\n",
       " 0.012786235,\n",
       " 0.07678006,\n",
       " 0.09305488,\n",
       " 0.05140529,\n",
       " 0.19781516,\n",
       " 0.03615095,\n",
       " 0.024941722,\n",
       " 0.29231974,\n",
       " 0.18815373,\n",
       " 1.5358394,\n",
       " 0.101616666,\n",
       " 0.08322685,\n",
       " 0.36795935,\n",
       " 0.30524057,\n",
       " 0.19538449,\n",
       " 0.05840248,\n",
       " 0.6108133,\n",
       " 0.07134169,\n",
       " 0.1872129,\n",
       " 0.014579032,\n",
       " 0.078923784,\n",
       " 0.16924319,\n",
       " 0.0,\n",
       " 0.023403494,\n",
       " 0.09478963,\n",
       " 0.16106832,\n",
       " 0.5541585,\n",
       " 0.032452766,\n",
       " 0.049134504,\n",
       " 0.068712,\n",
       " 0.037011437,\n",
       " 0.07341365,\n",
       " 0.7005012,\n",
       " 0.882526,\n",
       " 0.04158386,\n",
       " 0.13664801,\n",
       " 0.41066223,\n",
       " 0.65559363,\n",
       " 0.08534832,\n",
       " 0.028453356,\n",
       " 0.054910157,\n",
       " 0.19726892,\n",
       " 3.3386686,\n",
       " 0.104218975,\n",
       " 0.095837444,\n",
       " 0.0740985,\n",
       " 0.114025064,\n",
       " 0.13600698,\n",
       " 1.8141837,\n",
       " 0.01610733,\n",
       " 0.5019023,\n",
       " 0.009659222,\n",
       " 0.010588024,\n",
       " 0.13996826,\n",
       " 0.15487903,\n",
       " 0.6816381,\n",
       " 0.3153175,\n",
       " 0.03548163,\n",
       " 0.06679887,\n",
       " 0.31738797,\n",
       " 0.42260283,\n",
       " 0.02234928,\n",
       " 0.011840022,\n",
       " 0.054264616,\n",
       " 0.9305278,\n",
       " 0.17078547,\n",
       " 0.03169941,\n",
       " 0.6561735,\n",
       " 0.17738214,\n",
       " 0.051259443,\n",
       " 0.025210058,\n",
       " 0.7951182,\n",
       " 0.8990901,\n",
       " 0.010924532,\n",
       " 0.50991005,\n",
       " 0.1305417,\n",
       " 0.71148837,\n",
       " 0.13675205,\n",
       " 0.1723116,\n",
       " 0.04146222,\n",
       " 1.6135683,\n",
       " 0.45882577,\n",
       " 0.008523788,\n",
       " 0.22241272,\n",
       " 0.050032582,\n",
       " 2.3003485,\n",
       " 0.024561834,\n",
       " 0.32081696,\n",
       " 0.44927165,\n",
       " 0.008677744,\n",
       " 0.22658353,\n",
       " 0.040117923,\n",
       " 0.18306144,\n",
       " 0.12692434,\n",
       " 0.11595822,\n",
       " 0.25553262,\n",
       " 0.0052775526,\n",
       " 1.7580229,\n",
       " 0.12310098,\n",
       " 1.3748662,\n",
       " 0.06570925,\n",
       " 0.6135512,\n",
       " 0.049697883,\n",
       " 0.3830383,\n",
       " 0.030313,\n",
       " 0.049155246,\n",
       " 0.015089938,\n",
       " 0.6494656,\n",
       " 0.2781554,\n",
       " 0.007259181,\n",
       " 0.10447365,\n",
       " 0.012127029,\n",
       " 0.09728299,\n",
       " 0.068807386,\n",
       " 0.15733945,\n",
       " 0.053191908,\n",
       " 0.22210304,\n",
       " 0.35446483,\n",
       " 0.26168585,\n",
       " 1.6035163,\n",
       " 0.11826359,\n",
       " 0.1033969,\n",
       " 0.0708997,\n",
       " 0.0040292083,\n",
       " 0.8080516,\n",
       " 0.14435801,\n",
       " 0.10291476,\n",
       " 0.19016662,\n",
       " 0.18159401,\n",
       " 0.33671728,\n",
       " 0.55415297,\n",
       " 0.12015651,\n",
       " 0.4594299,\n",
       " 0.024112958,\n",
       " 0.45139605,\n",
       " 0.09113646,\n",
       " 0.12477021,\n",
       " 0.124919906,\n",
       " 0.46073633,\n",
       " 0.19905417,\n",
       " 0.045405623,\n",
       " 0.07990091,\n",
       " 3.587184,\n",
       " 0.09233229,\n",
       " 0.46061856,\n",
       " 1.2854323,\n",
       " 0.07091331,\n",
       " 0.23454078,\n",
       " 0.8148598,\n",
       " 3.5058672,\n",
       " 0.16255838,\n",
       " 0.64920783,\n",
       " 0.5475127,\n",
       " 1.1504756,\n",
       " 0.37104762,\n",
       " 0.47268626,\n",
       " 0.056631424,\n",
       " 0.792665,\n",
       " 0.05912235,\n",
       " 0.5006518,\n",
       " 0.278856,\n",
       " 0.44359618,\n",
       " 0.051744614,\n",
       " 0.11773295,\n",
       " 0.66933185,\n",
       " 0.2684262,\n",
       " 0.23312363,\n",
       " 0.3740209,\n",
       " 0.26176086,\n",
       " 0.015560008,\n",
       " 0.30524245,\n",
       " 0.07672389,\n",
       " 0.068580285,\n",
       " 0.58460784,\n",
       " 0.10617982,\n",
       " 0.3146539,\n",
       " 0.022731543,\n",
       " 0.007041952,\n",
       " 0.022984132,\n",
       " 0.27912083,\n",
       " 0.6669251,\n",
       " 0.17952792,\n",
       " 0.05228733,\n",
       " 0.00286025,\n",
       " 0.009850392,\n",
       " 0.13266322,\n",
       " 0.32491472,\n",
       " 0.34113488,\n",
       " 0.15310884,\n",
       " 0.007976474,\n",
       " 0.040976983,\n",
       " 0.1040075,\n",
       " 0.2470856,\n",
       " 0.03245885,\n",
       " 0.21908061,\n",
       " 0.43408436,\n",
       " 0.0739621,\n",
       " 0.08709951,\n",
       " 0.2765971,\n",
       " 0.37992457,\n",
       " 0.06283592,\n",
       " 0.4394364,\n",
       " 0.9164362,\n",
       " 0.11650323,\n",
       " 0.46495286,\n",
       " 0.93374974,\n",
       " 0.16633314,\n",
       " 0.008104898,\n",
       " 0.0359886,\n",
       " 0.17215401,\n",
       " 1.5200282,\n",
       " 0.13166004,\n",
       " 0.30881342,\n",
       " 0.22231181,\n",
       " 0.09937423,\n",
       " 0.30884796,\n",
       " 0.14113674,\n",
       " 0.09545954,\n",
       " 0.21473463,\n",
       " 0.1693694,\n",
       " 0.16593058,\n",
       " 0.16847841,\n",
       " 0.22661696,\n",
       " 0.012809084,\n",
       " 0.08194085,\n",
       " 0.01776414,\n",
       " 0.09815467,\n",
       " 0.0443619,\n",
       " 0.20482612,\n",
       " 0.20822851,\n",
       " 0.05877313,\n",
       " 0.30524048,\n",
       " 0.2782819,\n",
       " 0.3912991,\n",
       " 0.17934963,\n",
       " 0.025928335,\n",
       " 0.231252,\n",
       " 0.70361215,\n",
       " 0.094480716,\n",
       " 0.677811,\n",
       " 0.060832422,\n",
       " 0.23034623,\n",
       " 0.104048535,\n",
       " 0.037476204,\n",
       " 0.29844072,\n",
       " 0.014019259,\n",
       " 0.16205965,\n",
       " 0.010572126,\n",
       " 0.2631256,\n",
       " 0.27870196,\n",
       " 0.3764291,\n",
       " 0.3000429,\n",
       " 0.061000247,\n",
       " 0.00069334963,\n",
       " 0.38957888,\n",
       " 0.16190562,\n",
       " 0.22019446,\n",
       " 0.1287253,\n",
       " 0.03328588,\n",
       " 0.009641134,\n",
       " 0.18760169,\n",
       " 0.013862247,\n",
       " 0.10714455,\n",
       " 0.780242,\n",
       " 0.5929408,\n",
       " 0.0,\n",
       " 0.08447986,\n",
       " 0.34971812,\n",
       " 2.3901818,\n",
       " 0.100822106,\n",
       " 0.13938525,\n",
       " 0.06802448,\n",
       " 0.04601171,\n",
       " 0.004137956,\n",
       " 0.08238835,\n",
       " 0.18561429,\n",
       " 0.102712564,\n",
       " 0.23983048,\n",
       " 0.12855431,\n",
       " 0.50722593,\n",
       " 0.07566048,\n",
       " 0.0,\n",
       " 0.1273329,\n",
       " 0.1422844,\n",
       " 0.14893363,\n",
       " 0.7902319,\n",
       " 0.0038909537,\n",
       " 0.3328491,\n",
       " 0.11378206,\n",
       " 0.060945425,\n",
       " 0.135841,\n",
       " 0.0015046044,\n",
       " 0.1389825,\n",
       " 0.22960874,\n",
       " 0.1607455,\n",
       " 0.3229862,\n",
       " 2.4285421,\n",
       " 0.093375,\n",
       " 0.34956855,\n",
       " 0.305165,\n",
       " 0.057157412,\n",
       " 0.044311717,\n",
       " 0.08685862,\n",
       " 0.35209733,\n",
       " 0.21496983,\n",
       " 0.6166286,\n",
       " 1.3122568,\n",
       " 0.021936743,\n",
       " 0.83242506,\n",
       " 1.0122776,\n",
       " 0.33728126,\n",
       " 0.12704463,\n",
       " 0.03594261,\n",
       " 0.6295742,\n",
       " 0.12384847,\n",
       " 0.4905639,\n",
       " 0.06865778,\n",
       " 0.4248284,\n",
       " 0.24343154,\n",
       " 0.042420156,\n",
       " 0.30457607,\n",
       " 0.41942045,\n",
       " 0.08799415,\n",
       " 0.501634,\n",
       " 0.08245531,\n",
       " 0.26563868,\n",
       " 0.21174277,\n",
       " 0.3990465,\n",
       " 0.021951389,\n",
       " 0.2782497,\n",
       " 0.24536563,\n",
       " 0.046003472,\n",
       " 0.43017316,\n",
       " 0.013097538,\n",
       " 0.058178168,\n",
       " 0.8513493,\n",
       " 0.029641597,\n",
       " 0.15252389,\n",
       " 0.10488751,\n",
       " 0.10463468,\n",
       " 0.37020126,\n",
       " 0.22885244,\n",
       " 0.015561167,\n",
       " 0.34437868,\n",
       " 0.07912577,\n",
       " 0.11021434,\n",
       " 0.10796774,\n",
       " 0.38155735,\n",
       " 0.92320395,\n",
       " 0.31487244,\n",
       " 0.1500963,\n",
       " 0.036869004,\n",
       " 0.56104124,\n",
       " 0.52237815,\n",
       " 0.006051865,\n",
       " 0.46901742,\n",
       " 0.04730054,\n",
       " 1.1322968,\n",
       " 0.5570632,\n",
       " 0.21712954,\n",
       " 2.0102847,\n",
       " 0.21206029,\n",
       " 0.0878062,\n",
       " 0.3519596,\n",
       " 0.035700377,\n",
       " 1.6044185,\n",
       " 0.008420137,\n",
       " 0.018528748,\n",
       " 0.10968829,\n",
       " 0.13140965,\n",
       " 0.06482406,\n",
       " 0.38463527,\n",
       " 8.9124085e-05,\n",
       " 0.03704172,\n",
       " 0.044621006,\n",
       " 0.23209663,\n",
       " 0.164916,\n",
       " 0.11053049,\n",
       " 0.048848767,\n",
       " 0.12771428,\n",
       " 0.039245237,\n",
       " 0.17149377,\n",
       " 0.04839607,\n",
       " 0.0,\n",
       " 0.005268338,\n",
       " 0.6662992,\n",
       " 0.5574611,\n",
       " 0.051760774,\n",
       " 0.030345287,\n",
       " 0.10052131,\n",
       " 0.032762807,\n",
       " 0.060661275,\n",
       " 0.04049889,\n",
       " 0.0065568644,\n",
       " 0.17559709,\n",
       " 0.21635927,\n",
       " 0.020542687,\n",
       " 0.008601802,\n",
       " 0.056453224,\n",
       " 0.6335763,\n",
       " 0.18985917,\n",
       " 0.079999864,\n",
       " 0.0920698,\n",
       " 5.0877514,\n",
       " 0.3663166,\n",
       " 0.07259174,\n",
       " 0.35912487,\n",
       " 0.14244267,\n",
       " 1.0745568,\n",
       " 0.10687262,\n",
       " 0.21882117,\n",
       " 0.06471315,\n",
       " 0.31401095,\n",
       " 0.1408921,\n",
       " 0.36146557,\n",
       " 0.24391362,\n",
       " 0.01706953,\n",
       " 0.08159061,\n",
       " 0.72512645,\n",
       " 0.13965166,\n",
       " 0.06874109,\n",
       " 0.0019853977,\n",
       " 0.8904373,\n",
       " 0.041409183,\n",
       " 0.13002901,\n",
       " 0.122032225,\n",
       " 0.0065681734,\n",
       " 0.020028003,\n",
       " 0.40338984,\n",
       " 0.004801542,\n",
       " 0.08235005,\n",
       " 0.011016786,\n",
       " 0.46036097,\n",
       " 0.17982611,\n",
       " 0.35853276,\n",
       " 0.8658392,\n",
       " 0.1320593,\n",
       " 0.3965067,\n",
       " 0.18356869,\n",
       " 4.8406396,\n",
       " 0.044324704,\n",
       " 0.0181921,\n",
       " 1.2631531,\n",
       " 0.033805206,\n",
       " 0.44820085,\n",
       " 0.12463353,\n",
       " 0.05725411,\n",
       " 0.2906947,\n",
       " 0.06442394,\n",
       " 0.006444483,\n",
       " 0.22418137,\n",
       " 0.54955053,\n",
       " 0.44956547,\n",
       " 0.030940523,\n",
       " 0.21055266,\n",
       " 0.05438322,\n",
       " 0.2679005,\n",
       " 0.015970644,\n",
       " 0.074989304,\n",
       " 0.08209614,\n",
       " 0.4154778,\n",
       " 0.6341226,\n",
       " 0.102774486,\n",
       " 0.0054178056,\n",
       " 0.00025128986,\n",
       " 0.012729924,\n",
       " 5.1294293,\n",
       " 0.21037415,\n",
       " 0.29512122,\n",
       " 2.945329,\n",
       " 0.108248904,\n",
       " 1.331903,\n",
       " 0.33303586,\n",
       " 0.09608307,\n",
       " 0.17949516,\n",
       " 0.424547,\n",
       " 0.5070165,\n",
       " 0.022265824,\n",
       " 0.1101153,\n",
       " 0.024698202,\n",
       " 0.057598412,\n",
       " 0.11496124,\n",
       " 0.5437237,\n",
       " 1.7797544,\n",
       " 0.10542507,\n",
       " 0.24671848,\n",
       " 0.0130627295,\n",
       " 0.16339734,\n",
       " 0.22632243,\n",
       " 0.20242433,\n",
       " 2.1046011,\n",
       " 0.0035045242,\n",
       " 0.052018624,\n",
       " 0.068992026,\n",
       " 0.2239627,\n",
       " 0.4188195,\n",
       " 0.01650667,\n",
       " 0.12885202,\n",
       " 0.68438256,\n",
       " 0.20300525,\n",
       " 0.08141186,\n",
       " 0.1678915,\n",
       " 0.24184862,\n",
       " 0.110079795,\n",
       " 0.033946488,\n",
       " 0.50957733,\n",
       " 0.05148329,\n",
       " 0.1192215,\n",
       " 2.3916016,\n",
       " 0.9424843,\n",
       " 0.14873786,\n",
       " 0.047035035,\n",
       " 0.06121561,\n",
       " 0.19020125,\n",
       " 1.4351661,\n",
       " 0.031507734,\n",
       " 0.21275876,\n",
       " 0.4071775,\n",
       " 0.8175031,\n",
       " 0.048228882,\n",
       " 0.03523503,\n",
       " 0.4708702,\n",
       " 0.10951725,\n",
       " 0.39974687,\n",
       " 0.049156774,\n",
       " 0.3495128,\n",
       " 0.14975506,\n",
       " 0.39931118,\n",
       " 0.2122892,\n",
       " 0.30207375,\n",
       " 0.14189188,\n",
       " 0.1000531,\n",
       " 0.20216408,\n",
       " 0.19795856,\n",
       " 0.5908503,\n",
       " 0.059461087,\n",
       " 0.14592306,\n",
       " 0.15602966,\n",
       " 1.1620383,\n",
       " 0.503084,\n",
       " 0.14469895,\n",
       " 0.066346444,\n",
       " 0.83210063,\n",
       " 0.098701455,\n",
       " 0.17433554,\n",
       " 0.008653888,\n",
       " 0.17250445,\n",
       " 0.012815165,\n",
       " 0.39862397,\n",
       " 1.1189064,\n",
       " 0.0028035531,\n",
       " 0.06085032,\n",
       " 0.22440864,\n",
       " 0.5520945,\n",
       " 0.021635236,\n",
       " 0.20078856,\n",
       " 0.21644533,\n",
       " 0.17741215,\n",
       " 0.13702098,\n",
       " 0.14781897,\n",
       " 0.11166804,\n",
       " 0.36037636,\n",
       " 0.16792116,\n",
       " 0.16868146,\n",
       " 0.9879667,\n",
       " 0.3643696,\n",
       " 0.044228215,\n",
       " 0.020610934,\n",
       " 0.22044541,\n",
       " 0.46579665,\n",
       " 0.12818387,\n",
       " 0.48056355,\n",
       " 0.13547361,\n",
       " 0.066590235,\n",
       " 0.22476324,\n",
       " 0.0,\n",
       " 0.05453307,\n",
       " 0.32851186,\n",
       " 0.075578354,\n",
       " 0.064894855,\n",
       " 0.17894895,\n",
       " 0.10472858,\n",
       " 0.046542566,\n",
       " 0.15991573,\n",
       " 0.19504094,\n",
       " 0.7793056,\n",
       " 0.2386407,\n",
       " 0.3725174,\n",
       " 0.004190633,\n",
       " 0.30653682,\n",
       " 0.40341645,\n",
       " 0.18665433,\n",
       " 0.022777367,\n",
       " 0.058882076,\n",
       " 0.024827037,\n",
       " 0.10693883,\n",
       " 0.098989405,\n",
       " 0.17820928,\n",
       " 0.78739583,\n",
       " 0.0985141,\n",
       " 0.42528725,\n",
       " 0.01900577,\n",
       " 0.18834227,\n",
       " 1.4492385,\n",
       " 0.36165917,\n",
       " 0.2354269,\n",
       " 0.2593652,\n",
       " 0.09431246,\n",
       " 0.08286261,\n",
       " 1.1582898,\n",
       " 0.28021705,\n",
       " 0.18133129,\n",
       " 1.3923519,\n",
       " 0.88567793,\n",
       " 0.12208017,\n",
       " 0.3233972,\n",
       " 0.215218,\n",
       " 0.035761423,\n",
       " 0.019231776,\n",
       " 0.17710266,\n",
       " 0.0040098676,\n",
       " 0.05792711,\n",
       " 0.9940849,\n",
       " 0.12775996,\n",
       " 0.20397325,\n",
       " 0.64472914,\n",
       " 0.55013573,\n",
       " 0.25974104,\n",
       " 0.34815833,\n",
       " 0.44653004,\n",
       " 0.4256533,\n",
       " 0.099067755,\n",
       " 0.13452183,\n",
       " 1.6257244,\n",
       " 0.35659972,\n",
       " 0.0059188,\n",
       " 0.5028511,\n",
       " 0.5460169,\n",
       " 0.01755421,\n",
       " 0.26575866,\n",
       " 0.42322493,\n",
       " 0.015390148,\n",
       " 0.4426104,\n",
       " 0.21514256,\n",
       " 0.04549744,\n",
       " 1.5197874,\n",
       " 0.14997365,\n",
       " 1.59117,\n",
       " 0.005736371,\n",
       " 0.031335592,\n",
       " 0.014719749,\n",
       " 0.004597639,\n",
       " 0.08861652,\n",
       " 0.60250044,\n",
       " 0.39079067,\n",
       " 0.11439303,\n",
       " 0.043857254,\n",
       " 0.0,\n",
       " 0.009247463,\n",
       " 0.21636763,\n",
       " 0.19623885,\n",
       " 0.024409078,\n",
       " 0.036113318,\n",
       " 0.16751422,\n",
       " 1.8182955,\n",
       " 0.065470755,\n",
       " 0.0686237,\n",
       " 0.65039825,\n",
       " 0.0063103945,\n",
       " 0.047413386,\n",
       " 0.04021762,\n",
       " 1.2917168,\n",
       " 0.33190477,\n",
       " 0.13047674,\n",
       " 0.30960974,\n",
       " 0.06951381,\n",
       " 0.091011114,\n",
       " 0.21839866,\n",
       " 0.002918811,\n",
       " 0.15239953,\n",
       " 0.3003411,\n",
       " 0.46117112,\n",
       " 0.031999633,\n",
       " 0.028179623,\n",
       " 0.19344921,\n",
       " 0.020194963,\n",
       " 1.3545215,\n",
       " 0.24395044,\n",
       " 0.1352309,\n",
       " 0.22415179,\n",
       " 0.6778114,\n",
       " 0.12327952,\n",
       " 0.021987507,\n",
       " 0.16339014,\n",
       " 0.91451585,\n",
       " 0.29760855,\n",
       " 1.1337352,\n",
       " 0.16501868,\n",
       " 0.49079132,\n",
       " 0.26399925,\n",
       " 0.1517073,\n",
       " 0.28936338,\n",
       " 0.31873167,\n",
       " 0.07387363,\n",
       " 0.42416,\n",
       " 0.81179196,\n",
       " 0.38041985,\n",
       " 1.7384923,\n",
       " 0.14970371,\n",
       " 2.6887586,\n",
       " 0.022362974,\n",
       " 0.02035799,\n",
       " 0.94399506,\n",
       " 0.07290915,\n",
       " 0.043385915,\n",
       " 0.089807086,\n",
       " 0.16040018,\n",
       " 0.21083756,\n",
       " 0.045235366,\n",
       " 0.21993476,\n",
       " 0.25719193,\n",
       " 0.09779701,\n",
       " 0.15633368,\n",
       " 0.460543,\n",
       " 0.59413594,\n",
       " 0.21388024,\n",
       " 0.06782221,\n",
       " 0.09431757,\n",
       " 0.004228087,\n",
       " 1.2761261,\n",
       " 0.32382154,\n",
       " 1.2221378,\n",
       " 0.57448554,\n",
       " 0.0,\n",
       " 7.337805e-05,\n",
       " 0.030062327,\n",
       " 0.14375739,\n",
       " 0.06367952,\n",
       " 0.006098964,\n",
       " 0.04836988,\n",
       " 0.20610864,\n",
       " 0.18381035,\n",
       " 0.37051302,\n",
       " 0.017001355,\n",
       " 0.36748514,\n",
       " 0.014878339,\n",
       " 0.122230984,\n",
       " 0.34822395,\n",
       " 0.08411018,\n",
       " 0.54304165,\n",
       " 0.03452186,\n",
       " 0.016187152,\n",
       " 0.17026031,\n",
       " 0.00108075,\n",
       " 3.7664735,\n",
       " 0.050176512,\n",
       " 1.059202,\n",
       " 0.10842969,\n",
       " 0.00074288296,\n",
       " 0.033715386,\n",
       " 0.5353742,\n",
       " 0.033217583,\n",
       " 0.009739433,\n",
       " 0.046195682,\n",
       " 0.04829233,\n",
       " 0.6426409,\n",
       " 0.07121114,\n",
       " 0.0,\n",
       " 1.090746,\n",
       " 0.012563016,\n",
       " 0.8002335,\n",
       " 0.18555856,\n",
       " 0.024742119,\n",
       " 0.0317266,\n",
       " 1.205494,\n",
       " 1.4891908,\n",
       " 0.03410054,\n",
       " 1.2868352,\n",
       " 0.87989366,\n",
       " 0.80274343,\n",
       " 0.122861356,\n",
       " 1.7357641,\n",
       " 0.034381412,\n",
       " 0.037474878,\n",
       " 0.43716264,\n",
       " 0.3005712,\n",
       " 0.016952833,\n",
       " 0.18674229,\n",
       " 0.009867013,\n",
       " 0.003612666,\n",
       " 0.082111426,\n",
       " 0.021156935,\n",
       " 0.04767571,\n",
       " 0.20739937,\n",
       " 0.37746575,\n",
       " 0.4925644,\n",
       " 0.22636369,\n",
       " 0.031108078,\n",
       " 0.53013515,\n",
       " 0.041924827,\n",
       " 0.45309356,\n",
       " 1.8808439,\n",
       " 0.110730454,\n",
       " 0.043518256,\n",
       " 0.059288245,\n",
       " 0.45275444,\n",
       " 0.006518035,\n",
       " 0.054372687,\n",
       " 0.4336918,\n",
       " 0.94166994,\n",
       " 0.7434404,\n",
       " 0.64203167,\n",
       " 0.19724298,\n",
       " 0.17601801,\n",
       " 0.27423304,\n",
       " 0.14637902,\n",
       " 0.1535705,\n",
       " 0.19053005,\n",
       " 0.016167274,\n",
       " 1.5292664,\n",
       " 0.2992765,\n",
       " 0.78529865,\n",
       " 0.16804132,\n",
       " 0.0,\n",
       " 0.08720527,\n",
       " 0.02865497,\n",
       " 0.28931683,\n",
       " 0.42387816,\n",
       " 0.01543874,\n",
       " 0.7501302,\n",
       " 0.25185174,\n",
       " 0.038688466,\n",
       " 0.042846475,\n",
       " 0.04043935,\n",
       " 0.068741776,\n",
       " 0.11016234,\n",
       " 0.9402429,\n",
       " 0.058498092,\n",
       " 0.040089663,\n",
       " 0.114135206,\n",
       " 0.60383314,\n",
       " 0.022592159,\n",
       " 0.286715,\n",
       " 0.08769917,\n",
       " 0.17387769,\n",
       " 1.7098173,\n",
       " 0.40454087,\n",
       " 0.14752185,\n",
       " 0.72037303,\n",
       " 0.10428448,\n",
       " 0.028362725,\n",
       " 0.06792657,\n",
       " 0.056000054,\n",
       " 0.24780138,\n",
       " 0.008999555,\n",
       " 0.527183,\n",
       " 0.346866,\n",
       " 0.11291417,\n",
       " 0.038104355,\n",
       " 0.11983962,\n",
       " 2.101455,\n",
       " 0.09916197,\n",
       " 0.6728745,\n",
       " 0.002526358,\n",
       " 0.09478778,\n",
       " 0.15638132,\n",
       " 0.17431802,\n",
       " 0.06989384,\n",
       " 0.017794874,\n",
       " 0.011816721,\n",
       " 1.7074189,\n",
       " 0.054260068,\n",
       " 1.020449,\n",
       " 0.4484745,\n",
       " 0.75481385,\n",
       " 0.046901196,\n",
       " 0.14833805,\n",
       " 0.087603085,\n",
       " 0.38153738,\n",
       " 0.56705654,\n",
       " 0.03870202,\n",
       " 0.24652195,\n",
       " 0.39878473,\n",
       " 1.3828437,\n",
       " 0.17927188,\n",
       " 0.44322467,\n",
       " 0.22026391,\n",
       " 0.25178695,\n",
       " 0.072473064,\n",
       " 0.037637874,\n",
       " 0.35060173,\n",
       " 0.025044339,\n",
       " 2.1761472,\n",
       " 0.22016391,\n",
       " 0.08164836,\n",
       " 0.26450744,\n",
       " 0.0196163,\n",
       " 0.033656757,\n",
       " 0.93758667,\n",
       " 0.090014644,\n",
       " 0.031301755,\n",
       " 0.05382566,\n",
       " 0.0009381752,\n",
       " 0.332521,\n",
       " 0.093354575,\n",
       " 0.45752007,\n",
       " 0.2098381,\n",
       " 0.4897373,\n",
       " 0.19622856,\n",
       " 0.036628142,\n",
       " 0.04683119,\n",
       " 0.24023697,\n",
       " 0.042632226,\n",
       " 1.5003396,\n",
       " 0.93222743,\n",
       " 0.13640814,\n",
       " 0.28274554,\n",
       " 0.59621453,\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat[:2048 + 2048*j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_PCA_prod(features, use_all=False, num_to_use=3, num_components=12):\n",
    "    \n",
    "    if use_all:\n",
    "        pca = PCA()\n",
    "        X_train_PCA = pca.fit_transform(features)[:,:num_to_use]\n",
    "        X_test_PCA = pca.transform(features)[:,:num_to_use]\n",
    "\n",
    "        return X_train_PCA, X_test_PCA\n",
    "    \n",
    "    pca = PCA(num_components)\n",
    "    return pca.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(features):\n",
    "    predictions = []\n",
    "    \n",
    "    for j in range(5):\n",
    "        model_path = f'../app/random_forest_model_{j}.pkl'\n",
    "        file_path = f'../model/pca_model{j}.pkl'\n",
    "        \n",
    "        with open(file_path, 'rb') as file:\n",
    "            pca = pickle.load(file)\n",
    "        with open(model_path, 'rb') as file:\n",
    "            model = pickle.load(file)\n",
    "            \n",
    "        feature_vector = pca.transform(np.array(features[2048*j:2048 + 2048*j]).reshape(1,-1))\n",
    "        prediction = model.predict(np.array(feature_vector).reshape(1,-1))\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "    return stats.mode(predictions)[0][0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but PCA was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but PCA was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but PCA was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but PCA was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but PCA was fitted with feature names\n",
      "  warnings.warn(\n",
      "/var/folders/k_/y5wzmsx1483f9z9t9tjtdj1m0000gn/T/ipykernel_15381/2630694709.py:17: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  print(stats.mode(predictions)[0][0].squeeze())\n",
      "/var/folders/k_/y5wzmsx1483f9z9t9tjtdj1m0000gn/T/ipykernel_15381/2630694709.py:18: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  return stats.mode(predictions)[0][0].squeeze()\n"
     ]
    }
   ],
   "source": [
    "pred = run_model(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
